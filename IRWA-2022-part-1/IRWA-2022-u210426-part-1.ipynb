{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "046f0ad9",
   "metadata": {},
   "source": [
    "# IRWA-2022-u210426-part-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3242ec9",
   "metadata": {},
   "source": [
    "## 1) Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8e54dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0a706",
   "metadata": {},
   "source": [
    "After importing all modules I have to download the stop words from nltk module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70a2496e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Utente\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae3bc3",
   "metadata": {},
   "source": [
    "## 2) Load data and build map\n",
    "After the default part, I create a function load_data(path_tweets, path_docs_tweet) when path_tweets is the path to the file where are stored the tweets in the json format and path_docs_tweet is the path to another file that contains doc_id and the corrisponding tweet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4cdf8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_tweets, path_docs_tweet):\n",
    "    id_tweet = {}\n",
    "    doc_tweet = {}\n",
    "    with open(path_tweets) as tp:\n",
    "        for line in tp.readlines():\n",
    "            tweet = json.loads(line)\n",
    "            id_tweet[tweet['id']] = tweet\n",
    "\n",
    "    with open(path_docs_tweet) as dp:\n",
    "        for line in dp.readlines():\n",
    "            line = line.split()\n",
    "            doc_tweet[line[0]] = id_tweet[int(line[1])]\n",
    "    return doc_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4bb58a",
   "metadata": {},
   "source": [
    "Alter load data process we have to extract the information from the tweet to show it in the format:\n",
    "\n",
    "    Tweet | Username | Date | Hashtags | Likes | Retweets | Url\n",
    "    \n",
    "Where:\n",
    "- by tweet we mean the full text,\n",
    "- by username the screen_name of the user (so it identify the user),\n",
    "- by date we use the format 'name_day number_day name_month year'(e.g. Friday 30 September 2022)\n",
    "- by hashtags a string of the hashtags inside the text (double ##)\n",
    "- by likes the number of the likes\n",
    "- by retweets the number of retweets\n",
    "- by url the tweet link in the format 'https://twitter.com/username/status/tweet_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0a4d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(tweet):\n",
    "    try:\n",
    "        return tweet['full_text']\n",
    "    except KeyError:\n",
    "        return ' '\n",
    "\n",
    "def get_username(tweet):\n",
    "    try:\n",
    "        return tweet['user']['screen_name']\n",
    "    except KeyError:\n",
    "        return ' '\n",
    "\n",
    "def get_date(tweet):\n",
    "    try:\n",
    "        created_at = datetime.datetime.strptime(tweet['created_at'], \"%a %b %d %X %z %Y\" )\n",
    "        return created_at.strftime('%A %d %B %Y')\n",
    "    except KeyError:\n",
    "        return ' '\n",
    "\n",
    "def get_hashtags(tweet):\n",
    "    try:\n",
    "        hashtags = []\n",
    "        for hash in  tweet['entities']['hashtags']:\n",
    "                hashtags.append('##' + hash['text'])\n",
    "        return ' '.join(hashtags)\n",
    "    except KeyError:\n",
    "        return ' '\n",
    "\n",
    "def get_likes(tweet):\n",
    "    try:\n",
    "        return str(tweet['favorite_count'])\n",
    "    except KeyError:\n",
    "        return ' '\n",
    "\n",
    "def get_retweets(tweet):\n",
    "    try:\n",
    "        return str(tweet['retweet_count'])\n",
    "    except KeyError:\n",
    "        return ' '\n",
    "\n",
    "def get_url(tweet):\n",
    "    try:\n",
    "        return 'https://twitter.com/' + tweet['user']['screen_name'] + '/status/'+ str(tweet['id'])\n",
    "    except KeyError:\n",
    "        return ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dcd8a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_map(dict_docs_tweet):\n",
    "    doc_map = {}\n",
    "    for doc in dict_docs_tweet.keys():\n",
    "        tweet = dict_docs_tweet[doc]\n",
    "        \n",
    "        items_list = [get_text(tweet), get_username(tweet), get_date(tweet), get_hashtags(tweet), get_likes(tweet), get_retweets(tweet), get_url(tweet)] \n",
    "        doc_map[doc] = \" | \".join(items_list)\n",
    "        \n",
    "    return doc_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58895fe9",
   "metadata": {},
   "source": [
    "## 3) See some results\n",
    "\n",
    "I initialize the paths and than load the data using tha function created before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b0b8583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of docs of tweets: 4000\n"
     ]
    }
   ],
   "source": [
    "TWEETS_PATH = 'data/tw_hurricane_data.json'\n",
    "DOCS_PATH = 'data/tweet_document_ids_map.csv'\n",
    "doc_to_tweet = load_data(TWEETS_PATH, DOCS_PATH)\n",
    "print(\"Total number of docs of tweets: {}\".format(len(doc_to_tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f185e04e",
   "metadata": {},
   "source": [
    "In the next part I build the map of the future original data to show, and show some of the items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "44230105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original doc_1 text line:\n",
      "    So this will keep spinning over us until 7 pmâ€¦go away already. #HurricaneIan https://t.co/VROTxNS9rz | suzjdean | Friday 30 September 2022 | ##HurricaneIan | 0 | 0 | https://twitter.com/suzjdean/status/1575918182698979328 \n",
      "\n",
      "Original doc_2 text line:\n",
      "    Our hearts go out to all those affected by #HurricaneIan. We wish everyone on the roads currently braving the conditions safe travels. ðŸ’™ | lytx | Friday 30 September 2022 | ##HurricaneIan | 0 | 0 | https://twitter.com/lytx/status/1575918151862304768 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs_map = build_map(doc_to_tweet)\n",
    "for index in range(2):\n",
    "    doc = list(docs_map.keys())[index]\n",
    "    print(\"Original {} text line:\\n    {} \\n\".format(doc, docs_map[doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7179b258",
   "metadata": {},
   "source": [
    "## 4) Preprocess the text\n",
    "After creating the map we have to create another map with al the processed text. To do that I create another function that takes a string and transform it by:\n",
    "- Making all the text lower\n",
    "- Removing punctuation marksÃ¹\n",
    "- Removing links\n",
    "- Tokenize the text to get a list of terms\n",
    "- Eliminate the stopwords\n",
    "- Perform stemming\n",
    "\n",
    "I choose not to remove # and @ because I want that they appear different (like hashtags and users). However I just preprocess the hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "582ca8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(str_line):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "\n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    str_line = str_line.lower()\n",
    "    str_line = re.sub(r'(\\s)(#)[^\\s]+', , str_line) # Preprocess the hashtags\n",
    "    str_line = re.sub(r'(\\s)(http)[^\\s]+', ' ', str_line) # Removing links\n",
    "    str_line = re.sub(r'[^\\w\\s#@]+', ' ', str_line) # Removing punctuation marks\n",
    "    str_line = str_line.split()  # Tokenize the text to get a list of terms\n",
    "    str_line = [x for x in str_line if x not in stop_words]  # Eliminate the stopwords\n",
    "    str_line = [stemmer.stem(word) for word in str_line]  # Perform stemming\n",
    "    return str_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e794c40",
   "metadata": {},
   "source": [
    "Now I can make a map with preprocess text (I choose the tweet text, the username and date for the research):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a99dcae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prep_map(dict_docs_tweet):\n",
    "    prep_doc_map = {}\n",
    "    for doc in dict_docs_tweet.keys():\n",
    "        tweet = dict_docs_tweet[doc]\n",
    "        prep_doc_map[doc] = preprocess(get_text(tweet)) + preprocess(get_username(tweet)) + preprocess(get_date(tweet))\n",
    "    return prep_doc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4bc5a548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to preprocess tweets: 4.65 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "prep_docs_map = build_prep_map(doc_to_tweet)\n",
    "print(\"Total time to preprocess tweets: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52df1e",
   "metadata": {},
   "source": [
    "## 5) Final result of preprocessing\n",
    "Now all the text is ready to be insert in the index, but before we can just see some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d7e09e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess doc_1 text line:\n",
      "   ['keep', 'spin', 'us', '7', 'pm', 'go', 'away', 'alreadi', '#hurricaneian', 'suzjdean', 'friday', '30', 'septemb', '2022', '0', '0']\n",
      "\n",
      "Preprocess doc_2 text line:\n",
      "   ['heart', 'go', 'affect', '#hurricaneian', 'wish', 'everyon', 'road', 'current', 'brave', 'condit', 'safe', 'travel', 'lytx', 'friday', '30', 'septemb', '2022', '0', '0']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in range(2):\n",
    "    doc = list(prep_docs_map.keys())[index]\n",
    "    print(\"Preprocess {} text line:\\n   {}\\n\".format(doc, prep_docs_map[doc]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
