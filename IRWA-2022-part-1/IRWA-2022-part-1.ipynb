{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "046f0ad9",
   "metadata": {
    "id": "046f0ad9"
   },
   "source": [
    "# IRWA-2022-part-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p_CL2r-_itYh",
   "metadata": {
    "id": "p_CL2r-_itYh"
   },
   "source": [
    "# Introduction\n",
    "This is the notebook about the first part of the project of Information Retrival and Web Analysis. In this first part I have to load data from two files and preprocess it by removing stopwords, perform stemming and more...\n",
    "\n",
    "At the end of the project a user can create a dictionary with the doc_id as a key and all the information of the tweet as value by calling load_data(path_1, path_2).\n",
    "\n",
    "However, it's more usefull to not store all the information, but only some main information of tweet as values (in a specific format) by calling the function build_map() after the loading.\n",
    "\n",
    "\n",
    "Moreover the final step is to preprocess the text. To do that I have decided to create another function prep_build_map() that after the loading can create a dictionary with doc_id as key and some preprocess information about the tweet (only full text, username and date) as values. I want to leave the username with @ and hashtags with # as firsts characters because doing so they can easily be differentiated by other worlds (hashtag are anyway preprocess).\n",
    "\n",
    "I want to insert also username and date in the preprocessed text because I think they are important when I want to search a tweet (in the future steps of the project).\n",
    "\n",
    "In the ends there are some results of the preprocess function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-woukkG4l5tk",
   "metadata": {
    "id": "-woukkG4l5tk"
   },
   "source": [
    "# 0) Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WzGgT0KuiSxk",
   "metadata": {
    "id": "WzGgT0KuiSxk"
   },
   "source": [
    "I mount the google drive to use it in google colab, but it isn't necessary in git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9PNueamB905v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2126,
     "status": "ok",
     "timestamp": 1666341059842,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "9PNueamB905v",
    "outputId": "f1c83132-d9c6-483b-e374-17cbe32395d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "pBOa7E-w-CHp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1666341060259,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "pBOa7E-w-CHp",
    "outputId": "f3911328-8e76-466e-e069-72d09b892621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/IRWA/Project/IRWA-2022-part-1\n",
      "/content/drive/MyDrive/IRWA/Project/IRWA-2022-part-1\n",
      "data  IR_project_part_1.pdf  IRWA-2022-u210426-part-1.ipynb\n"
     ]
    }
   ],
   "source": [
    "#%cd /content/drive/MyDrive/IRWA/Project/IRWA-2022-part-1\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3242ec9",
   "metadata": {
    "id": "e3242ec9"
   },
   "source": [
    "## 1) Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e54dfe",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666341060259,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "d8e54dfe"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0a706",
   "metadata": {
    "id": "4fa0a706"
   },
   "source": [
    "After importing all modules I have to download the stop words from nltk module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70a2496e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1666341060260,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "70a2496e",
    "outputId": "d5607f35-4a10-4de2-9266-63a88fdc9862"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae3bc3",
   "metadata": {
    "id": "7aae3bc3"
   },
   "source": [
    "## 2) Load data and build map\n",
    "After the default part, I create a function load_data(path_tweets, path_docs_tweet) when path_tweets is the path to the file where are stored the tweets in the json format and path_docs_tweet is the path to another file that contains doc_id and the corrisponding tweet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cdf8f8c",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666341060260,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "4cdf8f8c"
   },
   "outputs": [],
   "source": [
    "def load_data(path_tweets, path_docs_tweet):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    path_tweets - path of the tweets file\n",
    "    path_docs_tweet - path of the 'doc_id tweet_id' file \n",
    "\n",
    "    Returns:\n",
    "    doc_tweet - dictionary of doc_id as keys and full tweets as values\n",
    "    \"\"\"\n",
    "\n",
    "    id_tweet = {}\n",
    "    doc_tweet = {}\n",
    "\n",
    "    #open the first path where are stored tweets\n",
    "    with open(path_tweets) as tp:\n",
    "        for line in tp.readlines():\n",
    "            #read every line with json.load() that create a dictionary of a single tweet\n",
    "            tweet = json.loads(line)\n",
    "            #store the tweet in a dictionary by tweet_id\n",
    "            id_tweet[tweet['id']] = tweet\n",
    "\n",
    "    #open the second path with doc_id - tweet_id\n",
    "    with open(path_docs_tweet) as dp:\n",
    "        for line in dp.readlines():\n",
    "            line = line.split()\n",
    "            #save the tweet from the dict of tweet in a new dict with doc_id as keys\n",
    "            doc_tweet[line[0]] = id_tweet[int(line[1])]\n",
    "    \n",
    "    #return the new dictionary\n",
    "    return doc_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4bb58a",
   "metadata": {
    "id": "9e4bb58a"
   },
   "source": [
    "Alter load data process we have to extract the information from the tweet to show it in the format:\n",
    "\n",
    "    Tweet | Username | Date | Hashtags | Likes | Retweets | Url\n",
    "    \n",
    "Where:\n",
    "- by tweet we mean the full text,\n",
    "- by username the screen_name of the user (so it identify the user),\n",
    "- by date we use the format 'name_day number_day name_month year'(e.g. Friday 30 September 2022)\n",
    "- by hashtags a string of the hashtags inside the text (double ##)\n",
    "- by likes the number of the likes\n",
    "- by retweets the number of retweets\n",
    "- by url the tweet link in the format 'https://twitter.com/username/status/tweet_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0a4d7b6",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666341060260,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "f0a4d7b6"
   },
   "outputs": [],
   "source": [
    "#get the text from a dictionary contains all informations of a tweet\n",
    "def get_text(tweet):\n",
    "    try:\n",
    "        return tweet['full_text']\n",
    "    except KeyError:\n",
    "        return ' '\n",
    "\n",
    "#get the username from a dictionary contains all informations of a tweet\n",
    "def get_username(tweet):\n",
    "    try:\n",
    "        #insert the @ to differentiate the username from other worlds\n",
    "        return '@' + tweet['user']['screen_name']\n",
    "    except KeyError:\n",
    "        return ' '\n",
    "\n",
    "\n",
    "#get the date from a dictionary contains all informations of a tweet\n",
    "def get_date(tweet):\n",
    "    try:\n",
    "        #change the format of the date to a easily readable one\n",
    "        created_at = datetime.datetime.strptime(tweet['created_at'], \"%a %b %d %X %z %Y\" )\n",
    "        return created_at.strftime('%A %d %B %Y')\n",
    "    except KeyError:\n",
    "        return ' '\n",
    "\n",
    "\n",
    "#get the hashtags from a dictionary contains all informations of a tweet\n",
    "def get_hashtags(tweet):\n",
    "    try:\n",
    "        hashtags = []\n",
    "        #read all the hashtags that are the values of a dictionary store in entities of the tweet\n",
    "        for hash in  tweet['entities']['hashtags']:\n",
    "                hashtags.append('#' + hash['text'])\n",
    "        #return hashtags as string\n",
    "        return ' '.join(hashtags)\n",
    "    except KeyError:\n",
    "        return ' '\n",
    "\n",
    "\n",
    "#get the number of likes from a dictionary contains all informations of a tweet\n",
    "def get_likes(tweet):\n",
    "    try:\n",
    "        return str(tweet['favorite_count'])\n",
    "    except KeyError:\n",
    "        return ' '\n",
    "\n",
    "\n",
    "#get the number of retweets from a dictionary contains all informations of a tweet\n",
    "def get_retweets(tweet):\n",
    "    try:\n",
    "        return str(tweet['retweet_count'])\n",
    "    except KeyError:\n",
    "        return ' '\n",
    "\n",
    "\n",
    "#get the url from a dictionary contains all informations of a tweet\n",
    "def get_url(tweet):\n",
    "    try:\n",
    "        #create the url by the username and the id of the tweet\n",
    "        return 'https://twitter.com/' + tweet['user']['screen_name'] + '/status/'+ str(tweet['id'])\n",
    "    except KeyError:\n",
    "        return ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcd8a429",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666341060261,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "dcd8a429"
   },
   "outputs": [],
   "source": [
    "#create the map of 'doc_id : tweet_as_string'\n",
    "def build_map(dict_docs_tweet):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    dict_docs_tweet -- dictionary of doc_id as keys and all tweets as values\n",
    "\n",
    "    Returns:\n",
    "    doc_map - new dictionary of doc_id as keys and main information of tweets as string as values\n",
    "    \"\"\"\n",
    "\n",
    "    doc_map = {}\n",
    "    for doc in dict_docs_tweet.keys():\n",
    "        tweet = dict_docs_tweet[doc]\n",
    "        \n",
    "        #insert all elements in a list\n",
    "        items_list = [get_text(tweet), get_username(tweet), get_date(tweet), get_hashtags(tweet), get_likes(tweet), get_retweets(tweet), get_url(tweet)] \n",
    "        #save the list as a string separated by |\n",
    "        doc_map[doc] = \" | \".join(items_list)\n",
    "        \n",
    "    return doc_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58895fe9",
   "metadata": {
    "id": "58895fe9"
   },
   "source": [
    "## 3) See some results\n",
    "\n",
    "I initialize the paths and than load the data using tha function created before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b0b8583",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 568,
     "status": "ok",
     "timestamp": 1666341060824,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "7b0b8583",
    "outputId": "04dd0505-ea44-49a1-cdea-b5dc596631f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of docs of tweets: 4000\n"
     ]
    }
   ],
   "source": [
    "#save paths\n",
    "data_path = './../data/'\n",
    "TWEETS_PATH = data_path + 'tw_hurricane_data.json'\n",
    "DOCS_PATH = data_path + 'tweet_document_ids_map.csv'\n",
    "\n",
    "#load data\n",
    "doc_to_tweet = load_data(TWEETS_PATH, DOCS_PATH)\n",
    "\n",
    "#print the number of documents in the data\n",
    "print(\"Total number of docs of tweets: {}\".format(len(doc_to_tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f185e04e",
   "metadata": {
    "id": "f185e04e"
   },
   "source": [
    "In the next part I build the map of the future original data to show, and show some of the items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44230105",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1666341060824,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "44230105",
    "outputId": "86fa11cc-65ca-4160-9cda-b13e891a3aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original doc_1 text line:\n",
      "    So this will keep spinning over us until 7 pm…go away already. #HurricaneIan https://t.co/VROTxNS9rz | @suzjdean | Friday 30 September 2022 | #HurricaneIan | 0 | 0 | https://twitter.com/suzjdean/status/1575918182698979328 \n",
      "\n",
      "Original doc_2 text line:\n",
      "    Our hearts go out to all those affected by #HurricaneIan. We wish everyone on the roads currently braving the conditions safe travels. 💙 | @lytx | Friday 30 September 2022 | #HurricaneIan | 0 | 0 | https://twitter.com/lytx/status/1575918151862304768 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#build the map with tweet as string\n",
    "docs_map = build_map(doc_to_tweet)\n",
    "\n",
    "#see some results of original tweets\n",
    "for index in range(2):\n",
    "    doc = list(docs_map.keys())[index]\n",
    "    print(\"Original {} text line:\\n    {} \\n\".format(doc, docs_map[doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7179b258",
   "metadata": {
    "id": "7179b258"
   },
   "source": [
    "## 4) Preprocess the text\n",
    "After creating the map we have to create another map with al the processed text. To do that I create another function that takes a string and transform it by:\n",
    "- Making all the text lower\n",
    "- Removing punctuation marks\n",
    "- Preprocess the hashtags\n",
    "- Removing links\n",
    "- Tokenize the text to get a list of terms\n",
    "- Eliminate the stopwords\n",
    "- Perform stemming\n",
    "\n",
    "I choose not to remove # and @ because I want that they appear different (like hashtags and users). However I just preprocess the hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "582ca8a3",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1666341060824,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "582ca8a3"
   },
   "outputs": [],
   "source": [
    "# preprocess function\n",
    "def preprocess(str_line):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "\n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    stemmer = PorterStemmer() #save the stemmer\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\")) #main language english\n",
    "\n",
    "    str_line = str_line.lower() # case insensitive\n",
    "\n",
    "    #find hashtags in the text and preprocess it\n",
    "    for hashtag in re.findall(r'#([^\\s]+)', str_line): \n",
    "      str_line = str_line.replace(hashtag, ''.join(preprocess(hashtag)))  # Preprocess the hashtags\n",
    "    \n",
    "    str_line = re.sub(r'(\\s)(http)[^\\s]+', ' ', str_line) # Removing links\n",
    "    str_line = re.sub(r'[^\\w\\s#@]+', ' ', str_line) # Removing punctuation marks\n",
    "    str_line = str_line.split()  # Tokenize the text to get a list of terms\n",
    "    str_line = [x for x in str_line if x not in stop_words]  # Eliminate the stopwords\n",
    "    str_line = [stemmer.stem(word) for word in str_line]  # Perform stemming\n",
    "    return str_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e794c40",
   "metadata": {
    "id": "3e794c40"
   },
   "source": [
    "Now I can make a map with preprocess text (I choose the tweet text, the username and date for the research):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a99dcae4",
   "metadata": {
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1666341061159,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "a99dcae4"
   },
   "outputs": [],
   "source": [
    "def build_prep_map(dict_docs_tweet):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    dict_docs_tweet -- dictionary of doc_id as keys and all tweets as values\n",
    "\n",
    "    Returns:\n",
    "    doc_map - new dictionary of doc_id as keys and preprocessed text, username and date as unique string as values\n",
    "    \"\"\"\n",
    "\n",
    "    prep_doc_map = {}\n",
    "    for doc in dict_docs_tweet.keys():\n",
    "        tweet = dict_docs_tweet[doc]\n",
    "        #preprocess every items and store into the dictionary\n",
    "        prep_doc_map[doc] = preprocess(get_text(tweet)) + preprocess(get_username(tweet)) + preprocess(get_date(tweet))\n",
    "    return prep_doc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bc5a548",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7554,
     "status": "ok",
     "timestamp": 1666341068710,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "4bc5a548",
    "outputId": "3cff2dcb-4e9a-40cc-a361-b5e418c90786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to preprocess tweets: 7.67 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "prep_docs_map = build_prep_map(doc_to_tweet)\n",
    "print(\"Total time to preprocess tweets: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52df1e",
   "metadata": {
    "id": "3d52df1e"
   },
   "source": [
    "## 5) Final result of preprocessing\n",
    "Now all the text is ready to be insert in the index, but before we can just see some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7e09e6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1666341068711,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "d7e09e6b",
    "outputId": "b58d5792-7079-4cb1-f80a-6f0b92b31e4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess doc_1 text line:\n",
      "   ['keep', 'spin', 'us', '7', 'pm', 'go', 'away', 'alreadi', '#hurricaneian', '@suzjdean', 'friday', '30', 'septemb', '2022']\n",
      "\n",
      "Preprocess doc_2 text line:\n",
      "   ['heart', 'go', 'affect', '#hurricaneian', 'wish', 'everyon', 'road', 'current', 'brave', 'condit', 'safe', 'travel', '@lytx', 'friday', '30', 'septemb', '2022']\n",
      "\n",
      "Preprocess doc_3 text line:\n",
      "   ['kissimme', 'neighborhood', 'michigan', 'ave', '#hurricaneian', '@cheathwftv', 'friday', '30', 'septemb', '2022']\n",
      "\n",
      "Preprocess doc_4 text line:\n",
      "   ['one', 'tree', 'backyard', 'scare', 'poltergeist', 'tree', 'storm', 'windi', 'like', '#scwx', '#hurricaneian', '@spiralgypsi', 'friday', '30', 'septemb', '2022']\n",
      "\n",
      "Preprocess doc_5 text line:\n",
      "   ['@ashleyruizwx', '@stephan89441722', '@lilmizzheidi', '@mr__sniffl', '@winknew', '@dylanfedericowx', '@julianamwx', '@sydneypers', '@nicolegabetv', 'pray', 'everyon', 'affect', '#hurricaneian', 'associ', 'winknew', 'sympathi', 'anim', 'abus', 'liar', 'condon', '@blondie610', 'friday', '30', 'septemb', '2022']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in range(5):\n",
    "    doc = list(prep_docs_map.keys())[index]\n",
    "    print(\"Preprocess {} text line:\\n   {}\\n\".format(doc, prep_docs_map[doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2QAfD8YXrMmL",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1666341202614,
     "user": {
      "displayName": "CRISTIAN BASSOTTO",
      "userId": "08684350298838062342"
     },
     "user_tz": -120
    },
    "id": "2QAfD8YXrMmL"
   },
   "outputs": [],
   "source": [
    "#end of the part-1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "492de5c55b0784f5394852be3d129468b1a3cbd17c8e16230d794b01878ae870"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
