{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2009a40c",
   "metadata": {},
   "source": [
    "Nil Agell u172941\n",
    "\n",
    "Jordi Badia u173484\n",
    "\n",
    "Cristian Bassotto u210426"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b29d290",
   "metadata": {},
   "source": [
    "# PART 1 - Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d19f9",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "edec1693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Utente\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d1d20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634b795",
   "metadata": {},
   "source": [
    "## Opening and loading files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68a1be",
   "metadata": {},
   "source": [
    "We open the json document and read line by line. Once the lines have been read, we load the json and we can verify that there are a total of 4000 tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e0020c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 4000\n"
     ]
    }
   ],
   "source": [
    "docs_path = '/Users/jordi/Documents/UNI/4rt Curs/1r Trim/IRWA/P1-IRWA/tw_hurricane_data.json'\n",
    "docs_path = './tw_hurricane_data.json'\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "\n",
    "print('Number of tweets:', len(lines))\n",
    "lines = [json.loads(l) for l in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba002b",
   "metadata": {},
   "source": [
    "We also open the csv document, which will allow us to map the tweets id with the document ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "74340a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3369: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>1575918182698979328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_2</td>\n",
       "      <td>1575918151862304768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_3</td>\n",
       "      <td>1575918140839673873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_4</td>\n",
       "      <td>1575918135009738752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_5</td>\n",
       "      <td>1575918119251419136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc                   id\n",
       "0  doc_1  1575918182698979328\n",
       "1  doc_2  1575918151862304768\n",
       "2  doc_3  1575918140839673873\n",
       "3  doc_4  1575918135009738752\n",
       "4  doc_5  1575918119251419136"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# define the dataset location\n",
    "filename = 'tweet_document_ids_map.csv'\n",
    "sep=\"\\t\"\n",
    "# Set Pandas to show all the columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Read the data as a dataframe\n",
    "data = pd.read_csv(filename,sep,names=[\"doc\", \"id\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443519a1",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ec934a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the tweet text removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line=  line.lower() ## Transform in lowercase\n",
    "    line=  line.split() ## Tokenize the text to get a list of terms\n",
    "    tweet_text=[]\n",
    "    for word in line:\n",
    "        #let's try to maintain the links in the correct format for the last part\n",
    "        if \"https\" not in word: #we maintain the # and @ because have relevance and we delete all the punctuation\n",
    "            word = re.sub(r'[^\\w\\s#@]','', word)\n",
    "            word = re.sub(r'_','',word)\n",
    "\n",
    "        if word:\n",
    "            tweet_text.append(word) \n",
    "            \n",
    "    line=[word for word in tweet_text if not word in stop_words]  ##eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line=[stemmer.stem(word) for word in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    ## END CODE\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7d98fd",
   "metadata": {},
   "source": [
    "# Part 2: Indexing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98255edf",
   "metadata": {},
   "source": [
    "## 2.1 Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b63620",
   "metadata": {},
   "source": [
    "### 2.1.1 Build inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab907dd5",
   "metadata": {},
   "source": [
    "We create the index tfidf (also provided with the class code ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e029d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(tweets, num_documents):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    num_documents -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df = defaultdict(int)  #document frequencies of terms in the input document\n",
    "    idf = defaultdict(float)\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tw_id = tweet['id']\n",
    "        terms = build_terms(tweet['full_text'])\n",
    "        for i in range(len(data)):\n",
    "            if data['id'][i] == tw_id:\n",
    "                doc_id = data['doc'][i]\n",
    "\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the **current page** and store it in current_page_index\n",
    "        ## current_doc_index ==> { ‚Äòterm1‚Äô: [current_doc, [list of positions]], ...,‚Äòterm_n‚Äô: [current_doc, [list of positions]]}\n",
    "\n",
    "\n",
    "        ## current_page_index ==> { ‚Äòweb‚Äô: [1, [0]], ‚Äòretrieval‚Äô: [1, [1,4]], ‚Äòinformation‚Äô: [1, [2]]}\n",
    "\n",
    "        ## the term ‚Äòweb‚Äô appears in document 1 in positions 0, \n",
    "        ## the term ‚Äòretrieval‚Äô appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_doc_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):  ## terms contains tweet text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                current_doc_index[term][1].append(position) \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_doc_index[term] = [doc_id, array('I', [position])]  #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        # normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_doc_index.items():\n",
    "            # posting will contain the list of positions for current term in current document. \n",
    "            # posting ==> [current_doc, [list of positions]] \n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_doc_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1]) / norm, 4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] += 1 # increment DF for current term\n",
    "\n",
    "        #merge the current doc index with the main index\n",
    "        for term_doc, posting_doc in current_doc_index.items():\n",
    "            index[term_doc].append(posting_doc)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_documents / df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab6fe7",
   "metadata": {},
   "source": [
    "We generate the new indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea7e3c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the TD-IDF index: 364.62 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "num_documents = len(lines)\n",
    "index, tf, df, idf = create_index_tfidf(lines, num_documents)\n",
    "print(\"Total time to create the TD-IDF index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc12936b",
   "metadata": {},
   "source": [
    "### 2.1.2 Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b84f71",
   "metadata": {},
   "source": [
    "We rank the documents with the provided function in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a9a4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(terms, docs, index, idf, tf):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        # TODO: check how to vectorize the query\n",
    "        # query_vector[termIndex]=idf[term]  # original\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex] = query_terms_count[term] / query_norm * idf[term]\n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "    \n",
    "    doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return result_docs, doc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71534a97",
   "metadata": {},
   "source": [
    "#### Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c5034",
   "metadata": {},
   "source": [
    "We get the ranked list of docs by getting the id of the docs that contain the query term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ce8877ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs = [posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs |= set(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs, doc_scores = rank_documents(query, docs, index, idf, tf)\n",
    "    return ranked_docs, doc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3052e0ad",
   "metadata": {},
   "source": [
    "We create an auxiliary function to list the hashtags of a tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "392c079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_hashtags(tweet):\n",
    "    hashtags = []\n",
    "    for tag in tweet[\"entities\"][\"hashtags\"]: #we loop the hashtags of the tweet that we can find in tweet[\"entities\"][\"hashtags\"]\n",
    "        hashtags.append(\"#\"+tag[\"text\"])\n",
    "    return ' '.join(hashtags).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0259c7",
   "metadata": {},
   "source": [
    "#### Tweet display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8f633",
   "metadata": {},
   "source": [
    "We create a function to display the relevant information of the tweet ( we also added the document to see it works properly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2088a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_display(tweets, idd,data):\n",
    "    for i in range(len(data)):\n",
    "        if data['doc'][i] == idd: #we map the doc_id to the tweet_id\n",
    "            doc = data['doc'][i]\n",
    "            tw_id = data['id'][i]\n",
    "            for tweet in tweets: #we loop all tweets until we find the one with the corresponding tweet_id, then we display\n",
    "                if (tweet['id']) == tw_id:\n",
    "                    return\"Document : \"+ str(doc)+  \" | \" +\" Tweet: \" + str(tweet['full_text']) + \" | \" + \"Username: \" + str(tweet[\"user\"][\"name\"]) + \" | \" + \"Date: \"+ str(tweet[\"created_at\"]) + \" | \" + \"Hashtags: \" + list_hashtags(tweet) + \" | \" + \"Likes: \" +  str(tweet[\"favorite_count\"]) + \" | \" + \"Retweets: \"+ str(tweet[\"retweet_count\"]) + \" | \" + \"Url: \" + \"twitter.com/\"+str(tweet[\"user\"][\"id\"])+\"/status/\"+tweet['id_str']+\"\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20158424",
   "metadata": {},
   "source": [
    "Here we have an example of a possible query and its results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0182426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      " Computer Science\n",
      "\n",
      "======================\n",
      "Top 10 results out of 7 for the searched query:\n",
      "\n",
      "Document : doc_264 |  Tweet: Oh‚Ä¶so NOW Floridians believe the science and listen to the advice of government officials. Good to know! #HurricaneIan | Username: Will Luera | Date: Fri Sep 30 18:25:44 +0000 2022 | Hashtags: #HurricaneIan | Likes: 2 | Retweets: 0 | Url: twitter.com/7652422/status/1575914809383583744\n",
      "\n",
      "Document : doc_2308 |  Tweet: As we await the effects of #HurricaneIan, here's a look at the science behind predicting hurricanes and why it's so tricky.\n",
      "\n",
      "https://t.co/LyOe8J3Bx0 | Username: Working@Duke | Date: Fri Sep 30 16:01:12 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 0 | Url: twitter.com/246912867/status/1575878436207443969\n",
      "\n",
      "Document : doc_1798 |  Tweet: Stuck in WA state due to #hurricaneian - Damage assessment has commenced through the use of @CrisisTrack on my 12-yr old‚Äôs school computer. üòÇ a week trip turned into a two week trip. #paradisefloodie https://t.co/KiCwlOYIRy | Username: CFM Flood Mgr | Date: Fri Sep 30 16:45:21 +0000 2022 | Hashtags: #hurricaneian #paradisefloodie | Likes: 1 | Retweets: 0 | Url: twitter.com/849291804390309889/status/1575889546101035008\n",
      "\n",
      "Document : doc_2653 |  Tweet: My point is hurricanes are becoming MORE stronger and powerful.\n",
      "We know it and we see it.\n",
      "And mostly we know why.\n",
      "Science.\n",
      "We have the evidence.\n",
      "We have temperatures increasing in Earth's atmosphere.\n",
      "We have the receipts. üåÄüåßüåç\n",
      "#ClimateChange \n",
      "#HurricaneIan\n",
      "#Florida\n",
      "@MSNBC @CNN | Username: Jennifer Coleman | Date: Fri Sep 30 15:42:16 +0000 2022 | Hashtags: #ClimateChange #HurricaneIan #Florida | Likes: 1 | Retweets: 1 | Url: twitter.com/2218000892/status/1575873674413867008\n",
      "\n",
      "Document : doc_3331 |  Tweet: #JunkScience \"study\" sez \"man-caused\" #GlobalWarming added 10% rain to #HurricaneIan?\n",
      "‚Ä¢Computer simulation? Same scam that saw 2.2Œú #Covid dead in US?\n",
      "‚Ä¢Gov't lab\n",
      "‚Ä¢Not peer-reviewed\n",
      "‚Ä¢Pimped by #FakeNews @AP \n",
      "‚Ä¢You'd buy eco-dictatorship \"to save earth\"?\n",
      "https://t.co/ztid0sqLhK | Username: The Fraser Faithful | Date: Fri Sep 30 15:08:32 +0000 2022 | Hashtags: #JunkScience #GlobalWarming #HurricaneIan #Covid #FakeNews | Likes: 2 | Retweets: 2 | Url: twitter.com/875111535517028357/status/1575865182944894978\n",
      "\n",
      "Document : doc_3787 |  Tweet: üåÄCharles, a student @SevenOaksSCmm, was inspired by #HurricaneIan to learn about the science behind the the tropical storm system! Thank you for inspiring all of us to be curious and pick up a book to find the answer, Charles! #D5Reads365 #OurD5Story https://t.co/RcdPkMLVtN | Username: Lex-Rich 5 Schools | Date: Fri Sep 30 14:44:10 +0000 2022 | Hashtags: #HurricaneIan #D5Reads365 #OurD5Story | Likes: 3 | Retweets: 0 | Url: twitter.com/49703393/status/1575859051233038341\n",
      "\n",
      "Document : doc_3900 |  Tweet: HURRICANE IAN RECOVERY: Christian Tech Center Ministries is pleased to announce the launch of our #HurricaneIan tech recovery program to assist local households affected by this storm. If your computer has been damaged or is no longer working as a result of Ian's impact, https://t.co/WIofQW4nzc | Username: Christian Tech Center Ministries | Date: Fri Sep 30 14:38:22 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 0 | Url: twitter.com/1333552175004475397/status/1575857591602712576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_docs, doc_scores = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(tweet_display(lines, d_id,data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c892ef",
   "metadata": {},
   "source": [
    "### 2.1.3 Test queries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474747ce",
   "metadata": {},
   "source": [
    "The dataset concern a hurrican, so we have decided for the test query 5 words that are related to this phenonim like flood, hurrican, landfall, emergency (because is a natural disaster) and Florida (for the last big hurrican Ian).\n",
    "\n",
    "These are the most frequent terms in the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525aae1e",
   "metadata": {},
   "source": [
    "#### Test 1: Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "12ab8ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 10 results out of 261 for the searched query:\n",
      "\n",
      "Document : doc_1493 |  Tweet: It‚Äôs not the wind that‚Äôs so bad for us in the lowcountry ; like in Florida it‚Äôs the wind I worry about. Here it‚Äôs the FLOODING. The lowcountry floods during regular rain storms; hurricanes can flood out so many homes so fast here. #HurricaneIan | Username: qaatilüê∫ | #BlackRiddler üé≤ | Date: Fri Sep 30 17:14:34 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 0 | Url: twitter.com/958535964/status/1575896898481053697\n",
      "\n",
      "Document : doc_2488 |  Tweet: If you are in a flood zone, you will be required by your mortgage company to carry a flood policy.  Here is what it covers. \n",
      "#HurricaneIan #floodinsurance https://t.co/FEiq7SI4cq | Username: Tony Tyan | Date: Fri Sep 30 15:51:01 +0000 2022 | Hashtags: #HurricaneIan #floodinsurance | Likes: 0 | Retweets: 0 | Url: twitter.com/1551356616/status/1575875872934232064\n",
      "\n",
      "Document : doc_1862 |  Tweet: This is near where I live. Flood didn't affect me. Caused by flooding of #EconRiver #EconTrail #HurricaneIan #FloridaLife https://t.co/G1r0teVGAj | Username: Nydia needs coffee #FullofCoffee ‚òïüáµüá∑ | Date: Fri Sep 30 16:38:49 +0000 2022 | Hashtags: #EconRiver #EconTrail #HurricaneIan #FloridaLife | Likes: 7 | Retweets: 2 | Url: twitter.com/935229740851646464/status/1575887905234776064\n",
      "\n",
      "Document : doc_1691 |  Tweet: Flood Safety: If you‚Äôve been affected by flooding, please be aware of floodwater contaminants!\n",
      "\n",
      "‚ùåDo not drink floodwater \n",
      "‚ùåDo not cook, clean, or brush teeth with flood water \n",
      "‚úîÔ∏èCover open wounds \n",
      "‚úîÔ∏èLimit exposure to floodwater\n",
      "\n",
      "#HurricaneIan https://t.co/GPIF3Oj7mP | Username: Healthcare Ready | Date: Fri Sep 30 16:56:05 +0000 2022 | Hashtags: #HurricaneIan | Likes: 1 | Retweets: 2 | Url: twitter.com/83664766/status/1575892248319119360\n",
      "\n",
      "Document : doc_3960 |  Tweet: Flash Flood warning for coastal Georgetown county until 1:30 pm. 2\"-4\" of rain has already fallen with an additional 2\" to 3\" possible. Flash flooding is ongoing or expected to begin shortly. Be safe and NEVER travel/drive through flooded streets. #SCwx .@WBTWNews13 #HurricaneIan https://t.co/A5CmYPg3tc | Username: James Hopkins | Date: Fri Sep 30 14:34:58 +0000 2022 | Hashtags: #SCwx #HurricaneIan | Likes: 5 | Retweets: 3 | Url: twitter.com/19933511/status/1575856735734190081\n",
      "\n",
      "Document : doc_857 |  Tweet: Flooding outside Iona. #HurricaneIan https://t.co/ZIMzpJE2ge | Username: Kimberly Leonard | Date: Fri Sep 30 17:59:56 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 0 | Url: twitter.com/52282776/status/1575908317553213442\n",
      "\n",
      "Document : doc_1672 |  Tweet: Edgewater Florida Flooding. #HurricaneIan https://t.co/eK3lFQ0yLq | Username: tuddle | Date: Fri Sep 30 16:58:21 +0000 2022 | Hashtags: #HurricaneIan | Likes: 4 | Retweets: 0 | Url: twitter.com/15170752/status/1575892817770999808\n",
      "\n",
      "Document : doc_3488 |  Tweet: Power is staring to flicker on and off and the pool is about to flood over #HurricaneIan | Username: Eva | Date: Fri Sep 30 15:01:13 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 0 | Url: twitter.com/394258538/status/1575863343080157185\n",
      "\n",
      "Document : doc_3471 |  Tweet: #HurricaneIan Flooded Angelo Dawkins' Home \n",
      "\n",
      "https://t.co/DFqk1X1dgZ | Username: Ringside News | Date: Fri Sep 30 15:02:11 +0000 2022 | Hashtags: #HurricaneIan | Likes: 73 | Retweets: 8 | Url: twitter.com/1163427092/status/1575863585452048384\n",
      "\n",
      "Document : doc_3470 |  Tweet: #HurricaneIan Flooded Angelo Dawkins' Home \n",
      "\n",
      "https://t.co/XKve9Y21oo | Username: ‚≠êÔ∏èPWM - WWE & AEW News & Rumors‚≠êÔ∏è | Date: Fri Sep 30 15:02:11 +0000 2022 | Hashtags: #HurricaneIan | Likes: 3 | Retweets: 0 | Url: twitter.com/200978998/status/1575863585460420608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "query = 'flood'\n",
    "ranked_docs, doc_scores = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(tweet_display(lines, d_id,data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd5363",
   "metadata": {},
   "source": [
    "#### Test 2 : Emergency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b3796c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 10 results out of 95 for the searched query:\n",
      "\n",
      "Document : doc_2682 |  Tweet: Our thoughts and prayers are with the people of Florida. Members, please continue to check your email for important updates from the VTA about emergency waivers and @GovernorVA 's state of emergency declaration as we prepare for #HurricaneIan.\n",
      "https://t.co/BXXJMxed3J | Username: Va Trucking Assn. | Date: Fri Sep 30 15:40:56 +0000 2022 | Hashtags: #HurricaneIan | Likes: 1 | Retweets: 1 | Url: twitter.com/80934149/status/1575873336596647937\n",
      "\n",
      "Document : doc_588 |  Tweet: Disaster-Preparedness Training Expert and author, Haskell Moore, says, ‚Äúsave your phone for emergency communications and have two trusty flashlights minimum is my advice in any emergency,‚Äù Read more on the blog. https://t.co/Ajy4q4KZ5O\n",
      "#maglite\n",
      "#flashlite\n",
      "#hurricaneian | Username: MAGLITE | Date: Fri Sep 30 18:10:01 +0000 2022 | Hashtags: #maglite #flashlite #hurricaneian | Likes: 2 | Retweets: 1 | Url: twitter.com/2774222486/status/1575910855274905614\n",
      "\n",
      "Document : doc_1911 |  Tweet: Ilham Thurston of LSC‚Äôs Disaster Team is in Raleigh responding to Hurricane Ian. The Emergency Operations Center has been activated, and Ilham will be on call to respond alongside North Carolina Emergency Management. #LSC #hurricaneian https://t.co/ps50f4y6HC | Username: Lutheran Services Carolinas | Date: Fri Sep 30 16:34:56 +0000 2022 | Hashtags: #LSC #hurricaneian | Likes: 0 | Retweets: 1 | Url: twitter.com/1192108853632098304/status/1575886925600169984\n",
      "\n",
      "Document : doc_582 |  Tweet: @NWS South Carolinian's\n",
      "Follow your states Emergency Advisories\n",
      "&amp; Remember \"Turn Around Don't Drown\"\n",
      "\n",
      "State of Emergency for\n",
      "S Carolina @SCEMD\n",
      "SC Dept of Trans @SCDOTPress\n",
      "\n",
      "@NHC_Atlantic\n",
      "@fema\n",
      "@RedCross\n",
      "\n",
      "#HurricaneIan \n",
      "#hurricanequestions \n",
      "#Ian\n",
      "https://t.co/VronBdajgh | Username: JCTrevino | Date: Fri Sep 30 18:10:12 +0000 2022 | Hashtags: #HurricaneIan #hurricanequestions #Ian | Likes: 1 | Retweets: 0 | Url: twitter.com/1288241341986754561/status/1575910899634032640\n",
      "\n",
      "Document : doc_1674 |  Tweet: @NWSCharlestonSC @SCEMD South Carolinian's\n",
      "Follow your states Emergency Advisories\n",
      "&amp; Remember \"Turn Around Don't Drown\"\n",
      "\n",
      "State of Emergency for\n",
      "S Carolina @SCEMD\n",
      "@NHC_Atlantic\n",
      "@fema\n",
      "@RedCross\n",
      " @horrycountypd\n",
      "\n",
      "#HurricaneIan \n",
      "#hurricanequestions \n",
      "#Ian\n",
      "https://t.co/djw9wX8chL | Username: JCTrevino | Date: Fri Sep 30 16:58:11 +0000 2022 | Hashtags: #HurricaneIan #hurricanequestions #Ian | Likes: 0 | Retweets: 0 | Url: twitter.com/1288241341986754561/status/1575892777048494080\n",
      "\n",
      "Document : doc_143 |  Tweet: @EdPiotrowski South Carolinian's\n",
      "Follow your states Emergency Advisories\n",
      "&amp; Remember \"Turn Around Don't Drown\"\n",
      "\n",
      "State of Emergency for\n",
      "S Carolina @SCEMD\n",
      "SC Dept of Trans @SCDOTPress\n",
      "\n",
      "@NHC_Atlantic\n",
      "@fema\n",
      "@RedCross\n",
      "\n",
      "#HurricaneIan \n",
      "#hurricanequestions \n",
      "#Ian\n",
      "https://t.co/y3cxdlWiJp | Username: JCTrevino | Date: Fri Sep 30 18:31:31 +0000 2022 | Hashtags: #HurricaneIan #hurricanequestions #Ian | Likes: 0 | Retweets: 0 | Url: twitter.com/1288241341986754561/status/1575916265285513216\n",
      "\n",
      "Document : doc_1882 |  Tweet: @NWSSPC Please tune into your local weather \n",
      "Follow your states Emergency Advisories\n",
      "\n",
      "States of Emergency for\n",
      "Georgia @GeorgiaEMAHS\n",
      "S Carolina @SCEMD\n",
      "N Carolina @NCPublicSafety\n",
      "Virginia @VDEM\n",
      "#HurricaneIan \n",
      "#hurricanequestions \n",
      "#Ian\n",
      "@NHC_Atlantic\n",
      "@fema\n",
      "@RedCross\n",
      "https://t.co/7GoiLh6BVe | Username: JCTrevino | Date: Fri Sep 30 16:37:18 +0000 2022 | Hashtags: #HurricaneIan #hurricanequestions #Ian | Likes: 0 | Retweets: 0 | Url: twitter.com/1288241341986754561/status/1575887523074768896\n",
      "\n",
      "Document : doc_1685 |  Tweet: @horrycountypd @SCEMD South Carolinian's\n",
      "Follow your states Emergency Advisories\n",
      "&amp; Remember \"Turn Around Don't Drown\"\n",
      "\n",
      "State of Emergency for\n",
      "S Carolina @SCEMD\n",
      "\n",
      "@NHC_Atlantic\n",
      "@fema\n",
      "@RedCross \n",
      "@horrycountypd\n",
      "\n",
      "#HurricaneIan \n",
      "#hurricanequestions \n",
      "#Ian\n",
      "\n",
      "https://t.co/eIFnnCkkmy | Username: JCTrevino | Date: Fri Sep 30 16:56:35 +0000 2022 | Hashtags: #HurricaneIan #hurricanequestions #Ian | Likes: 0 | Retweets: 0 | Url: twitter.com/1288241341986754561/status/1575892373321572373\n",
      "\n",
      "Document : doc_2319 |  Tweet: FL - REGULATORY BULLETIN - Emergency Order: #HurricaneIan #PropertyLegalNews \n",
      "https://t.co/FdymEyXkhK https://t.co/7IUu3E3nj8 | Username: PLRB | Date: Fri Sep 30 16:00:43 +0000 2022 | Hashtags: #HurricaneIan #PropertyLegalNews | Likes: 0 | Retweets: 0 | Url: twitter.com/240695548/status/1575878315675717632\n",
      "\n",
      "Document : doc_375 |  Tweet: @horrycountypd @SCEMD South Carolinian's\n",
      "Follow your states Emergency Advisories\n",
      "Remember \"Turn Around Don't Drown\"\n",
      "\n",
      "State of Emergency for\n",
      "S Carolina @SCEMD\n",
      "SC Dept of Trans @SCDOTPress\n",
      "\n",
      "@horrycountypd\n",
      "@NHC_Atlantic \n",
      "@fema \n",
      "@RedCross\n",
      "\n",
      "#HurricaneIan \n",
      "#hurricanequestions \n",
      "#Ian\n",
      "https://t.co/pQgfQJYgh4 | Username: JCTrevino | Date: Fri Sep 30 18:20:58 +0000 2022 | Hashtags: #HurricaneIan #hurricanequestions #Ian | Likes: 1 | Retweets: 0 | Url: twitter.com/1288241341986754561/status/1575913610983133184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "query = 'emergency'\n",
    "ranked_docs, doc_scores = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(tweet_display(lines, d_id,data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aded266",
   "metadata": {},
   "source": [
    "#### Test 3 : Hurricane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9947c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 10 results out of 631 for the searched query:\n",
      "\n",
      "Document : doc_3218 |  Tweet: If the death doll isn't more than hurricane katrina, its pretty damn close. #HurricaneIan was absolutely worse than hurricane charley for Florida..and was THE STRONGEST hurricane in recorded history for our florida peninsula. | Username: Emily Bailee | Date: Fri Sep 30 15:14:25 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 0 | Url: twitter.com/45058299/status/1575866662070484994\n",
      "\n",
      "Document : doc_640 |  Tweet: Hurricane Ian before and after #HurricaneIan https://t.co/XZstkI2pN2 | Username: kaden fields ‚ú™ | Date: Fri Sep 30 18:07:50 +0000 2022 | Hashtags: #HurricaneIan | Likes: 2 | Retweets: 0 | Url: twitter.com/1329640781649416197/status/1575910304159977472\n",
      "\n",
      "Document : doc_3845 |  Tweet: So please y'all, hold off on the severe hurricane jokes because this hurricane isn't a joke right now. And it never will be. #hurricaneian | Username: UNDERCUT SEONGHWA ENTHUSIAST ‚ÄºÔ∏èü§© | Date: Fri Sep 30 14:41:02 +0000 2022 | Hashtags: #hurricaneian | Likes: 0 | Retweets: 0 | Url: twitter.com/1371237552737419265/status/1575858260837797888\n",
      "\n",
      "Document : doc_3336 |  Tweet: Although the Hurricane has finally passed, my thoughts and prayers go out to everyone trying to rebuild and recover from Hurricane Ian. \n",
      "\n",
      "#HurricaneIan üå™ | Username: Tuck Beck | Date: Fri Sep 30 15:07:56 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 1 | Url: twitter.com/278727661/status/1575865031207825408\n",
      "\n",
      "Document : doc_2537 |  Tweet: PRAY üôè FOR ALL WHO SUFFERED FROM THE HURRICANE #HurricaneIan | Username: Gamingforlife | Date: Fri Sep 30 15:48:48 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 0 | Url: twitter.com/1565775847960739841/status/1575875317918748679\n",
      "\n",
      "Document : doc_2140 |  Tweet: #hurricaneian\n",
      "The perpetual hurricane Ian | Username: üí•PermanentWave | Date: Fri Sep 30 16:14:34 +0000 2022 | Hashtags: #hurricaneian | Likes: 0 | Retweets: 0 | Url: twitter.com/75442215/status/1575881799267295234\n",
      "\n",
      "Document : doc_1217 |  Tweet: Hurricane Ian on tourüò≠\n",
      "#HurricaneIan | Username: New York City | Date: Fri Sep 30 17:37:34 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 0 | Url: twitter.com/1068652828661559297/status/1575902689040666626\n",
      "\n",
      "Document : doc_495 |  Tweet: Hurricane Ian be like \n",
      "\n",
      "‚ÄúYou all haven‚Äôt had any hurricanes around here lately so I‚Äôm just gonna take my time on the East Coast.‚Äù\n",
      "\n",
      "#HurricaneIan #Ian #Hurricane | Username: Correcting your üí© | Date: Fri Sep 30 18:14:39 +0000 2022 | Hashtags: #HurricaneIan #Ian #Hurricane | Likes: 0 | Retweets: 0 | Url: twitter.com/1565783598543806472/status/1575912020368449536\n",
      "\n",
      "Document : doc_1428 |  Tweet: BTW Florida's combative, anti-education/women/liberty gov DiSantis voted against hurricane relief money after Hurricane Sandy. think on that a moment. \n",
      "#HurricaneIan #Fla | Username: Butter Emailsüåªüåª | Date: Fri Sep 30 17:21:56 +0000 2022 | Hashtags: #HurricaneIan #Fla | Likes: 0 | Retweets: 0 | Url: twitter.com/17578329/status/1575898753826856960\n",
      "\n",
      "Document : doc_746 |  Tweet: Yesterday, Hurricane Ian made landfall near Cayo Costa, Fla., as a Category 4 hurricane, bringing record wind and storm surge to a four-county area in Southwest Florida. Read the full update on Hurricane Ian from our MWC Florida team here:\n",
      "\n",
      "#hurricaneian #florida | Username: McGuireWoods Consulting | Date: Fri Sep 30 18:03:17 +0000 2022 | Hashtags: #hurricaneian #florida | Likes: 0 | Retweets: 0 | Url: twitter.com/488644188/status/1575909158746038277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "query = 'hurricane'\n",
    "ranked_docs, doc_scores = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(tweet_display(lines, d_id,data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca8056e",
   "metadata": {},
   "source": [
    "#### Test 4 : Florida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3b238fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 10 results out of 547 for the searched query:\n",
      "\n",
      "Document : doc_2072 |  Tweet: Donate to the Florida Disaster Fund - Volunteer Florida to help Floridians recover from #hurricaneian  https://t.co/Ud4BNM81EJ | Username: üá∫üá∏ Amy‚ò¢Ô∏èREDACTED‚ò¢Ô∏è MAGArino ü§åüèªüá∫üá∏ | Date: Fri Sep 30 16:19:57 +0000 2022 | Hashtags: #hurricaneian | Likes: 1 | Retweets: 0 | Url: twitter.com/2398138445/status/1575883154153414656\n",
      "\n",
      "Document : doc_1700 |  Tweet: The current death toll in Florida sits at 21. About 10,000 people in southwest Florida are unaccounted for. üíî #hurricaneian | Username: School‚Äôs Out | Date: Fri Sep 30 16:55:15 +0000 2022 | Hashtags: #hurricaneian | Likes: 0 | Retweets: 0 | Url: twitter.com/1420132505546465280/status/1575892040541298690\n",
      "\n",
      "Document : doc_1701 |  Tweet: Update: #hurricaneian has passed Florida and is now headed to South Carolina.\n",
      "\n",
      "1.8 million remain without power in Florida. | Username: TheLatestBlock.com | Date: Fri Sep 30 16:55:12 +0000 2022 | Hashtags: #hurricaneian | Likes: 0 | Retweets: 0 | Url: twitter.com/1375638432341446656/status/1575892025312002048\n",
      "\n",
      "Document : doc_3964 |  Tweet: Duke Energy donates $100,000 to the Florida Disaster Fund, managed by Volunteer Florida Foundation, to assist communities affected by #HurricaneIan. To contribute to the Florida Disaster Fund, visit https://t.co/zx6l40CRQv or text DISASTER to 20222. Info: https://t.co/oAmCDqYkVQ https://t.co/UVsnxWHWGu | Username: Duke Energy | Date: Fri Sep 30 14:34:42 +0000 2022 | Hashtags: #HurricaneIan | Likes: 13 | Retweets: 11 | Url: twitter.com/81192849/status/1575856667304071170\n",
      "\n",
      "Document : doc_563 |  Tweet: Florida boss called Hurricane Ian a \"nothing burger\" ‚Äî urged staff to keep working: reports .. https://t.co/FOw04OW275::: Clearwater Florida #HurricaneIan https://t.co/zDbw2By1fk | Username: WTB Research | Date: Fri Sep 30 18:11:08 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 1 | Url: twitter.com/991187411630780416/status/1575911137937326081\n",
      "\n",
      "Document : doc_2506 |  Tweet: @MrMatthewTodd After, Florida is totally under water.\n",
      "Then, it will hit them. ü§î\n",
      "Climate change will take more and more costal lands.\n",
      "Florida is on it's way OUT. üåßüåÄ\n",
      "#HurricaneIan \n",
      "#GlobalWarming \n",
      "#ClimateChange \n",
      "#Florida \n",
      "#SouthCarolina | Username: Jennifer Coleman | Date: Fri Sep 30 15:50:08 +0000 2022 | Hashtags: #HurricaneIan #GlobalWarming #ClimateChange #Florida #SouthCarolina | Likes: 0 | Retweets: 0 | Url: twitter.com/2218000892/status/1575875654108811267\n",
      "\n",
      "Document : doc_2261 |  Tweet: @GovRonDeSantis said thousands of Florida residents will need help rebuilding after #hurricaneian and urged those that could to contribute to the Florida Disaster Fund ( https://t.co/th8d863roq ) https://t.co/yWKGgJ7hMm | Username: Chip Barnett | Date: Fri Sep 30 16:04:32 +0000 2022 | Hashtags: #hurricaneian | Likes: 0 | Retweets: 0 | Url: twitter.com/159096380/status/1575879277262032896\n",
      "\n",
      "Document : doc_3114 |  Tweet: Hey Florida..  lookie here..  #HurricaneIan https://t.co/JHJoHz5pd0 | Username: free2bhuman | Date: Fri Sep 30 15:19:53 +0000 2022 | Hashtags: #HurricaneIan | Likes: 1 | Retweets: 0 | Url: twitter.com/384077570/status/1575868040931323904\n",
      "\n",
      "Document : doc_2905 |  Tweet: Hurricane Ian heads towards South Carolina after striking Florida.\n",
      "The death toll in Florida has risen to 21. We are praying for all those affected. Our hearts are with you. #HurricaneIan https://t.co/RqvvtW7Mt5 | Username: lyfmail.com | Date: Fri Sep 30 15:29:30 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 0 | Url: twitter.com/1543603637817331713/status/1575870458964344833\n",
      "\n",
      "Document : doc_2899 |  Tweet: They are rebuilding Florida. \n",
      "Why? \n",
      "#ClimateEmergency #HurricaneIan https://t.co/XNaKn7m7lg | Username: üò∑Miriam O'Callaghan Has a Deadline | Date: Fri Sep 30 15:29:48 +0000 2022 | Hashtags: #ClimateEmergency #HurricaneIan | Likes: 4 | Retweets: 3 | Url: twitter.com/1031872658575642624/status/1575870535653011456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "query = 'florida'\n",
    "ranked_docs, doc_scores = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(tweet_display(lines, d_id,data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21521b9",
   "metadata": {},
   "source": [
    "#### Test 5 : Landfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "00805ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 10 results out of 238 for the searched query:\n",
      "\n",
      "Document : doc_399 |  Tweet: #HurricaneIan has made landfall! | Username: WeatherMAX National | Date: Fri Sep 30 18:19:30 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 0 | Url: twitter.com/1425534305619546118/status/1575913241116803072\n",
      "\n",
      "Document : doc_100 |  Tweet: Final #Ian and Charley comparison @HeatherZWeather, I promise! üòÇ\n",
      "\n",
      "After nearly identical landfall points in FL, #HurricaneIan's second landfall üåÄ in Georgetown, SC is only 24 miles from Charley's landfall in SC.\n",
      "\n",
      "@weatherchannel https://t.co/PvVOmy5Xhu | Username: Jason Disharoon | Date: Fri Sep 30 18:33:40 +0000 2022 | Hashtags: #Ian #HurricaneIan | Likes: 5 | Retweets: 1 | Url: twitter.com/37661008/status/1575916807927795712\n",
      "\n",
      "Document : doc_245 |  Tweet: 4th Landfall !!! #HurricaneIan https://t.co/RjDkz7HbEb | Username: Najam ŸÜÿ¨ŸÖ | Date: Fri Sep 30 18:26:30 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 0 | Url: twitter.com/712763496/status/1575915004590977024\n",
      "\n",
      "Document : doc_12 |  Tweet: #HurricaneIan officially made landfall at 2:15 pm near Georgetown, SC.\n",
      "\n",
      "Ian made landfall as a CAT 1 with sustained winds of 85 mph.\n",
      "\n",
      "This is Ian's 3rd landfall.\n",
      "\n",
      "@SpecNews1RDU \n",
      "\n",
      "@SpecNews1ILM \n",
      "\n",
      "@SpecNews1Triad \n",
      "\n",
      "@SpecNews1CLT \n",
      "\n",
      "@SpecNews1MTN https://t.co/BvcQeOLkBG | Username: Meteorologist Vernon Turner | Date: Fri Sep 30 18:38:34 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 0 | Url: twitter.com/3232685437/status/1575918038196666392\n",
      "\n",
      "Document : doc_314 |  Tweet: THE 3rd and Final Landfall of #HurricaneIan just occurred S of Myrtle Beach. The storm actually intensified as pressure dropped before landfall at 977mb. #scwx https://t.co/As1LR1r2vq | Username: I65Wx (BWG/SoKy) | Date: Fri Sep 30 18:23:34 +0000 2022 | Hashtags: #HurricaneIan #scwx | Likes: 2 | Retweets: 0 | Url: twitter.com/1371119451454763010/status/1575914266473140225\n",
      "\n",
      "Document : doc_386 |  Tweet: JUST IN: #HurricaneIan makes landfall near Georgetown. | Username: Kristina Lobo | Date: Fri Sep 30 18:20:07 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 0 | Url: twitter.com/1260754170/status/1575913396373012492\n",
      "\n",
      "Document : doc_3227 |  Tweet: #HurricaneIan is starting to make landfall here in #NC. | Username: OnTheMoon, AntiRugs.Eth | GetRichOrRuggedTryin.Eth | Date: Fri Sep 30 15:13:53 +0000 2022 | Hashtags: #HurricaneIan #NC | Likes: 2 | Retweets: 0 | Url: twitter.com/1395187526537826310/status/1575866530335629312\n",
      "\n",
      "Document : doc_1163 |  Tweet: Ian is making landfall again. #HurricaneIan https://t.co/NCxy8gqWLD | Username: Twisted Skies | Date: Fri Sep 30 17:40:35 +0000 2022 | Hashtags: #HurricaneIan | Likes: 2 | Retweets: 1 | Url: twitter.com/2856918817/status/1575903449405067264\n",
      "\n",
      "Document : doc_410 |  Tweet: #HurricaneIan just made landfall near Georgetown, SC | Username: Jennifer Van Laar | Date: Fri Sep 30 18:18:44 +0000 2022 | Hashtags: #HurricaneIan | Likes: 3 | Retweets: 0 | Url: twitter.com/775137023979958272/status/1575913047171813376\n",
      "\n",
      "Document : doc_3971 |  Tweet: #HurricaneIan as seen by Sentinel 3 just before landfall. https://t.co/b52i3g5blu | Username: Remy Mermelstein | WeatherInTheHud | Date: Fri Sep 30 14:34:19 +0000 2022 | Hashtags: #HurricaneIan | Likes: 0 | Retweets: 1 | Url: twitter.com/3438631077/status/1575856571783184385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "query = 'landfall'\n",
    "ranked_docs, doc_scores = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(tweet_display(lines, d_id,data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73623acd",
   "metadata": {},
   "source": [
    "## 2.2 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48bfbfe",
   "metadata": {},
   "source": [
    "### 2.2.1 Test relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e8ebf",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "85837622",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = pd.read_csv(\"evaluation_gt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "41ee9337",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results.rename(columns={'label': 'is_relevant'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a227ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_queries = ['Landfall in South Carolina', 'Help and recovery during the hurricane disaster', 'Floodings in South Carolina']\n",
    "\n",
    "predicted_relevance = []\n",
    "\n",
    "for pos, row in search_results.iterrows():\n",
    "    ranked_docs, doc_scores = search_tf_idf(base_queries[row['query_id']-1], index)\n",
    "    if row['doc'] in ranked_docs:\n",
    "        doc_index = ranked_docs.index(row['doc'])\n",
    "        predicted_relevance.append(doc_scores[doc_index][0])\n",
    "    else:\n",
    "        predicted_relevance.append(0)\n",
    "\n",
    "search_results['predicted_relevance'] = predicted_relevance\n",
    "baseline_df = search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e841e16e",
   "metadata": {},
   "source": [
    "#### Our evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aca3c869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_14228\\347492637.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#create the dataframe with our 5 queries to be evaluated\n",
    "our_queries = ['flood','emergency','hurricane','florida','landfall']\n",
    "\n",
    "query_results = pd.DataFrame(columns = [\"query_id\", \"doc_id\", \"predicted_relevance\"])\n",
    "\n",
    "for i in range(len(our_queries)):\n",
    "    ranked_docs, doc_scores = search_tf_idf(our_queries[i], index)\n",
    "    for j in range(len(ranked_docs[:10])):\n",
    "        query_results = query_results.append({\"query_id\": i+1, \"doc_id\": str(ranked_docs[j]), \"predicted_relevance\": doc_scores[j][0]}, ignore_index=True)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dc5d04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have assigned each doc to one relevance\n",
    "doc_score = [0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,0,1,1,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cf2ff607",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results[\"is_relevant\"] = doc_score\n",
    "#query_results[\"is_relevant\"] = query_results[\"doc_score\"].apply(lambda y: 1 if y >= 2 else 0)\n",
    "#query_results[\"predicted_bin_relevance\"] = query_results[\"predicted_relevance\"].apply(lambda y: 1 if y>=2 else 0)\n",
    "our_query_df = query_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a16952",
   "metadata": {},
   "source": [
    "#### Inspection of the two dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d33ea683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline relevance results: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>query_id</th>\n",
       "      <th>is_relevant</th>\n",
       "      <th>predicted_relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.267331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.453629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.294735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.149296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_501</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.437695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc_52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.083556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc_82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.567589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc_100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.400649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doc_122</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.788417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_165</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.806229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc  query_id  is_relevant  predicted_relevance\n",
       "0   doc_12         1            1             2.267331\n",
       "1    doc_9         1            1             1.453629\n",
       "2   doc_18         1            1             2.294735\n",
       "3   doc_45         1            1             1.149296\n",
       "4  doc_501         1            1             3.437695\n",
       "5   doc_52         1            1             1.083556\n",
       "6   doc_82         1            1             3.567589\n",
       "7  doc_100         1            1             2.400649\n",
       "8  doc_122         1            1             0.788417\n",
       "9  doc_165         1            1             2.806229"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our queries relevance results: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>is_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_1493</td>\n",
       "      <td>4.014152</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_2488</td>\n",
       "      <td>3.982116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_1862</td>\n",
       "      <td>3.982116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_1691</td>\n",
       "      <td>3.674424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_3960</td>\n",
       "      <td>3.579062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_857</td>\n",
       "      <td>3.331716</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_1672</td>\n",
       "      <td>3.331716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_3488</td>\n",
       "      <td>3.041159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_3471</td>\n",
       "      <td>3.041159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_3470</td>\n",
       "      <td>3.041159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id    doc_id predicted_relevance  is_relevant\n",
       "0        1  doc_1493            4.014152            0\n",
       "1        1  doc_2488            3.982116            1\n",
       "2        1  doc_1862            3.982116            1\n",
       "3        1  doc_1691            3.674424            1\n",
       "4        1  doc_3960            3.579062            1\n",
       "5        1   doc_857            3.331716            1\n",
       "6        1  doc_1672            3.331716            0\n",
       "7        1  doc_3488            3.041159            1\n",
       "8        1  doc_3471            3.041159            1\n",
       "9        1  doc_3470            3.041159            1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Baseline relevance results: ')\n",
    "display(baseline_df.head(10))\n",
    "\n",
    "print('Our queries relevance results: ')\n",
    "display(our_query_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa26a08",
   "metadata": {},
   "source": [
    "### 2.2.2 Evaluation algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5a8b4",
   "metadata": {},
   "source": [
    "#### Precision@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b6c858de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(doc_score, y_score, k=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    precision @k : float\n",
    "\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    doc_score = np.take(doc_score, order[:k]) #hay que ordenar los predicted igual que los del ground truth. Align binary relevance to the \n",
    "    relevant = sum(doc_score==1) #get number of relevant docs\n",
    "    return float(relevant)/k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf08bad6",
   "metadata": {},
   "source": [
    "#### Recall@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3ab1dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    " def recall_at_k(doc_score, y_score, k=10): #binary relevance, predicted relevance, k for a given query\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    recall @k : float\n",
    "\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1] \n",
    "    all_relevants = sum(doc_score == 1)\n",
    "    doc_score = np.take(doc_score, order[:k])\n",
    "    relevant = sum(doc_score == 1) \n",
    "    return float(relevant) / all_relevants #calculate recall at k, which is the number of relevant documents among all relevant docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c82de1",
   "metadata": {},
   "source": [
    "#### Average Precision@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b0bfe3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precision_at_k(doc_score, y_score, k=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    average precision @k : float\n",
    "    \"\"\"\n",
    "    gtp = np.sum(doc_score == 1) #Total number of gt positives\n",
    "    order = np.argsort(y_score)[::-1] #same as for precision\n",
    "    doc_score = np.take(doc_score, order[:k]) #same as for precision\n",
    "    ## if all documents are not relevant\n",
    "    if gtp == 0:\n",
    "        return 0\n",
    "    n_relevant_at_i = 0\n",
    "    prec_at_i = 0\n",
    "    for i in range(len(doc_score)):\n",
    "        if doc_score[i] == 1: #only add the P@k when the doc is relevant\n",
    "            n_relevant_at_i += 1\n",
    "            prec_at_i += n_relevant_at_i / (i + 1) #calculate P@K (#docs relevant at k/k)\n",
    "    return prec_at_i / gtp #return ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b705827",
   "metadata": {},
   "source": [
    "#### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "86c739c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(doc_score, y_score, k='inf'):\n",
    "    precision = precision_at_k(doc_score, y_score, k)\n",
    "    recall = recall_at_k(doc_score, y_score, k)\n",
    "    return 2*(precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7be36",
   "metadata": {},
   "source": [
    "#### Mean Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f6ba440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(search_res, k=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_res: search results dataset containing:\n",
    "        query_id: query id.\n",
    "        doc_id: document id.\n",
    "        predicted_relevance: relevance predicted through LightGBM.\n",
    "        doc_score: actual score of the document for the query (ground truth).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean average precision @ k : float\n",
    "    \"\"\"\n",
    "    avp = []\n",
    "    for q in search_res[\"query_id\"].unique():  # loop over all query id\n",
    "        curr_data =  search_res[search_res.query_id==q]# select data for current query\n",
    "        avp.append(avg_precision_at_k(np.array(curr_data[\"is_relevant\"]), \n",
    "                                      np.array(curr_data[\"predicted_relevance\"]),k ))  #append average precision for current query\n",
    "    return np.sum(avp)/len(avp), avp  # return mean average precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099e9a2",
   "metadata": {},
   "source": [
    "#### Mean Reciprocal Rank (MRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "07aa7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_at_k(doc_score, y_score, k=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Reciprocal Rank for qurrent query\n",
    "    \"\"\"\n",
    "\n",
    "    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order. As before\n",
    "    doc_score = np.take(doc_score, order[\n",
    "                             :k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k. As before\n",
    "    if np.sum(doc_score) == 0:  # if there are not relevant doument return 0\n",
    "        return 0\n",
    "    return 1 / (np.argmax(doc_score == 1) + 1)  # hint: to get the position of the first relevant document use \"np.argmax\" (+1 because the idex starts from 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "40a8dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(search_results,k = 10):\n",
    "    RRs = []\n",
    "    for q in search_results['query_id'].unique():  # loop over all query ids, get rrs for each query at each k\n",
    "        labels = np.array(search_results[search_results['query_id'] == q][\"is_relevant\"])  # get labels for current query\n",
    "        scores = np.array(search_results[search_results['query_id'] == q][\"predicted_relevance\"])  # get predicted score for current query\n",
    "        RRs.append(rr_at_k(labels, scores, k))  # append RR for current query\n",
    "    return np.round(float(sum(RRs) / len(RRs)), 5)  # Mean RR at current k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee82a8",
   "metadata": {},
   "source": [
    "#### Normalized Discounted Cumulative Gain (NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7d750ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(doc_score, y_score, k=10): #doc_scire are the labels (ground truth) and y_score are the system scores\n",
    "    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order.\n",
    "    doc_score = np.take(doc_score, order[:k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    gain = 2 ** doc_score - 1  # First we calculate the upper part of the formula which is the CG (use formula 7 above) (notice it is based on the ground truth relevance)\n",
    "    discounts = np.log2(np.arange(len(doc_score)) + 2)  # Compute denominator (np.arrange creates a list of numbers betweeen 0 and len(doc_score)-1), then the + 2 addresses the fact that the numbers start from 0\n",
    "    return np.sum(gain / discounts)  #return dcg@k\n",
    "\n",
    "\n",
    "def ndcg_at_k(doc_score, y_score, k=10):\n",
    "    dcg_max = dcg_at_k(doc_score, doc_score, k) #ideal dcg\n",
    "    #print(dcg_max)\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return np.round(dcg_at_k(doc_score, y_score, k) / dcg_max, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff508272",
   "metadata": {},
   "source": [
    "#### All evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e217a1",
   "metadata": {},
   "source": [
    "Evaluation of baseline queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6a91fe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query : Landfall in South Carolina\n",
      "==> Precision@5: 1.0\n",
      "==> Recall@5: 0.5\n",
      "==> Average Precision@5: 0.5\n",
      "==> F1-score of first 5: 0.6666666666666666\n",
      "==> NDCG@5: 1.0\n",
      "Check on the dataset sorted by score:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>query_id</th>\n",
       "      <th>is_relevant</th>\n",
       "      <th>predicted_relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc_82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.567589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_501</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.437695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc_165</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.806229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc_100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.400649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.294735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc  query_id  is_relevant  predicted_relevance\n",
       "6   doc_82         1            1             3.567589\n",
       "4  doc_501         1            1             3.437695\n",
       "9  doc_165         1            1             2.806229\n",
       "7  doc_100         1            1             2.400649\n",
       "2   doc_18         1            1             2.294735"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query : Help and recovery during the hurricane disaster\n",
      "==> Precision@5: 0.8\n",
      "==> Recall@5: 0.4\n",
      "==> Average Precision@5: 0.38\n",
      "==> F1-score of first 5: 0.5333333333333333\n",
      "==> NDCG@5: 0.8539\n",
      "Check on the dataset sorted by score:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>query_id</th>\n",
       "      <th>is_relevant</th>\n",
       "      <th>predicted_relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>doc_402</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.949660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>doc_268</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.360185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doc_504</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.852534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>doc_1233</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.299460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>doc_175</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.282269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doc  query_id  is_relevant  predicted_relevance\n",
       "17   doc_402         2            1             2.949660\n",
       "12   doc_268         2            1             2.360185\n",
       "19   doc_504         2            1             1.852534\n",
       "49  doc_1233         2            0             1.299460\n",
       "11   doc_175         2            1             1.282269"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query : Floodings in South Carolina\n",
      "==> Precision@5: 1.0\n",
      "==> Recall@5: 0.5\n",
      "==> Average Precision@5: 0.5\n",
      "==> F1-score of first 5: 0.6666666666666666\n",
      "==> NDCG@5: 1.0\n",
      "Check on the dataset sorted by score:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>query_id</th>\n",
       "      <th>is_relevant</th>\n",
       "      <th>predicted_relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>doc_148</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.079364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>doc_65</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.809531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>doc_65</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.809531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>doc_66</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.208867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>doc_30</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.123482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc  query_id  is_relevant  predicted_relevance\n",
       "24  doc_148         3            1             3.079364\n",
       "29   doc_65         3            1             2.809531\n",
       "21   doc_65         3            1             2.809531\n",
       "22   doc_66         3            1             2.208867\n",
       "20   doc_30         3            1             2.123482"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "==> Mean Average Precision (MAP) @5: 0.45999999999999996\n",
      "==> Mean Reciprocal Rank (MRR) @5: 1.0\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "for i in range(len(base_queries)):\n",
    "    current_query = i+1\n",
    "    current_query_res = baseline_df[baseline_df[\"query_id\"] == current_query]\n",
    "    \n",
    "    \n",
    "    print(\"Query :\", base_queries[i])\n",
    "    print(\"==> Precision@{}: {}\".format(k, precision_at_k(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "    print(\"==> Recall@{}: {}\".format(k, recall_at_k(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "    print(\"==> Average Precision@{}: {}\".format(k, avg_precision_at_k(np.array(current_query_res[\"is_relevant\"]), np.array(current_query_res[\"predicted_relevance\"]), k)))\n",
    "    print(\"==> F1-score of first {}: {}\".format(k, f1_score(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "    \n",
    "    print(\"==> NDCG@{}: {}\".format(k, ndcg_at_k(np.array(current_query_res[\"is_relevant\"]), np.array(current_query_res[\"predicted_relevance\"]), k)))\n",
    "    \n",
    "    print(\"Check on the dataset sorted by score:\")\n",
    "    \n",
    "    current_query_res = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "    display(current_query_res)\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "print(\"==> Mean Average Precision (MAP) @{}: {}\".format(k, map_at_k(baseline_df, k)[0]))\n",
    "print(\"==> Mean Reciprocal Rank (MRR) @{}: {}\".format(k, mrr(baseline_df, k)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439b414",
   "metadata": {},
   "source": [
    "Evaluation of our queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "34b6724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: flood\n",
      "==> Precision@5: 0.8\n",
      "==> Recall@5: 0.5\n",
      "==> Average Precision@5: 0.33958333333333335\n",
      "==> F1-score of first 5: 0.6153846153846154\n",
      "==> NDCG@5: 0.6608\n",
      "Check on the dataset sorted by score:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>is_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_1493</td>\n",
       "      <td>4.014152</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_2488</td>\n",
       "      <td>3.982116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_1862</td>\n",
       "      <td>3.982116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_1691</td>\n",
       "      <td>3.674424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>doc_3960</td>\n",
       "      <td>3.579062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id    doc_id predicted_relevance  is_relevant\n",
       "0        1  doc_1493            4.014152            0\n",
       "1        1  doc_2488            3.982116            1\n",
       "2        1  doc_1862            3.982116            1\n",
       "3        1  doc_1691            3.674424            1\n",
       "4        1  doc_3960            3.579062            1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2: emergency\n",
      "==> Precision@5: 0.4\n",
      "==> Recall@5: 0.2857142857142857\n",
      "==> Average Precision@5: 0.19999999999999998\n",
      "==> F1-score of first 5: 0.3333333333333333\n",
      "==> NDCG@5: 0.4704\n",
      "Check on the dataset sorted by score:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>is_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>doc_2682</td>\n",
       "      <td>5.833453</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>doc_588</td>\n",
       "      <td>5.595638</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>doc_1911</td>\n",
       "      <td>5.107419</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>doc_582</td>\n",
       "      <td>4.946544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>doc_1674</td>\n",
       "      <td>4.946544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id    doc_id predicted_relevance  is_relevant\n",
       "10        2  doc_2682            5.833453            1\n",
       "11        2   doc_588            5.595638            0\n",
       "12        2  doc_1911            5.107419            0\n",
       "13        2   doc_582            4.946544            1\n",
       "14        2  doc_1674            4.946544            1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 3: hurricane\n",
      "==> Precision@5: 0.4\n",
      "==> Recall@5: 0.4\n",
      "==> Average Precision@5: 0.18\n",
      "==> F1-score of first 5: 0.4000000000000001\n",
      "==> NDCG@5: 0.3452\n",
      "Check on the dataset sorted by score:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>is_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>doc_3218</td>\n",
       "      <td>2.00628</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>doc_640</td>\n",
       "      <td>1.70515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>doc_3845</td>\n",
       "      <td>1.70515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>doc_3336</td>\n",
       "      <td>1.70515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>doc_2537</td>\n",
       "      <td>1.70515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id    doc_id predicted_relevance  is_relevant\n",
       "20        3  doc_3218             2.00628            0\n",
       "21        3   doc_640             1.70515            0\n",
       "22        3  doc_3845             1.70515            1\n",
       "23        3  doc_3336             1.70515            1\n",
       "24        3  doc_2537             1.70515            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 4: florida\n",
      "==> Precision@5: 0.6\n",
      "==> Recall@5: 0.6\n",
      "==> Average Precision@5: 0.2866666666666666\n",
      "==> F1-score of first 5: 0.6\n",
      "==> NDCG@5: 0.4469\n",
      "Check on the dataset sorted by score:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>is_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>doc_2072</td>\n",
       "      <td>2.195784</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>doc_1700</td>\n",
       "      <td>2.115823</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>doc_1701</td>\n",
       "      <td>2.044174</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>doc_3964</td>\n",
       "      <td>1.854561</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4</td>\n",
       "      <td>doc_563</td>\n",
       "      <td>1.816164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id    doc_id predicted_relevance  is_relevant\n",
       "30        4  doc_2072            2.195784            0\n",
       "31        4  doc_1700            2.115823            0\n",
       "32        4  doc_1701            2.044174            1\n",
       "33        4  doc_3964            1.854561            1\n",
       "34        4   doc_563            1.816164            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 5: landfall\n",
      "==> Precision@5: 0.8\n",
      "==> Recall@5: 0.5714285714285714\n",
      "==> Average Precision@5: 0.5714285714285714\n",
      "==> F1-score of first 5: 0.6666666666666666\n",
      "==> NDCG@5: 0.8688\n",
      "Check on the dataset sorted by score:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>is_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5</td>\n",
       "      <td>doc_399</td>\n",
       "      <td>4.597579</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5</td>\n",
       "      <td>doc_100</td>\n",
       "      <td>4.158046</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5</td>\n",
       "      <td>doc_245</td>\n",
       "      <td>3.981278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5</td>\n",
       "      <td>doc_12</td>\n",
       "      <td>3.927132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5</td>\n",
       "      <td>doc_314</td>\n",
       "      <td>3.753549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id   doc_id predicted_relevance  is_relevant\n",
       "40        5  doc_399            4.597579            1\n",
       "41        5  doc_100            4.158046            1\n",
       "42        5  doc_245            3.981278            1\n",
       "43        5   doc_12            3.927132            1\n",
       "44        5  doc_314            3.753549            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "==> Mean Average Precision (MAP) @5: 0.31553571428571425\n",
      "==> Mean Reciprocal Rank (MRR) @5: 0.66667\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(our_queries)):\n",
    "    current_query = i+1\n",
    "    current_query_res = our_query_df[our_query_df[\"query_id\"] == current_query]\n",
    "    \n",
    "    print(\"Query {}:\".format(i+1), our_queries[i])\n",
    "    print(\"==> Precision@{}: {}\".format(k, precision_at_k(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "    print(\"==> Recall@{}: {}\".format(k, recall_at_k(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "    print(\"==> Average Precision@{}: {}\".format(k, avg_precision_at_k(np.array(current_query_res[\"is_relevant\"]), np.array(current_query_res[\"predicted_relevance\"]), k)))\n",
    "    print(\"==> F1-score of first {}: {}\".format(k, f1_score(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "    \n",
    "    print(\"==> NDCG@{}: {}\".format(k, ndcg_at_k(np.array(current_query_res[\"is_relevant\"]), np.array(current_query_res[\"predicted_relevance\"]), k)))\n",
    "    \n",
    "    print(\"Check on the dataset sorted by score:\")\n",
    "    \n",
    "    current_query_res = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "    display(current_query_res)\n",
    "    \n",
    "print('----------------------------------------------------')\n",
    "print(\"==> Mean Average Precision (MAP) @{}: {}\".format(k, map_at_k(our_query_df, k)[0]))\n",
    "print(\"==> Mean Reciprocal Rank (MRR) @{}: {}\".format(k, mrr(our_query_df, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3e5e7",
   "metadata": {},
   "source": [
    "### 2.2.3 Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db2a7e",
   "metadata": {},
   "source": [
    "Here, we represent the tweet words in 2 dimensions. First of all we take each word by processing the tweet text and create a word2vec model. Then we vectorize the tweet by taking the average value over the words involved. \n",
    "\n",
    "After all the tweets are vectorized we plot the vectors of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cf98a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tweets, model):\n",
    "    vecs_tweets = []\n",
    "    for tweet in tweets:\n",
    "        terms = build_terms(tweet['full_text'])\n",
    "        values = []\n",
    "        for term in terms:\n",
    "            # we calculate the value of every term thanks to the model and insert in values\n",
    "            values.append(model.wv[term])\n",
    "        \n",
    "        # for each tweet we calculate the average vector and insert it in the vec_tweets \n",
    "        vecs_tweets.append(sum(values)/len(values))\n",
    "    \n",
    "    #after vetorize all the tweets we return the list of vectors \n",
    "    return vecs_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "459cfc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Utente\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare corpus of sentences separed in terms\n",
    "corpus = []\n",
    "for tweet in lines:\n",
    "  terms = build_terms(tweet['full_text'])\n",
    "  corpus.append(terms)\n",
    "\n",
    "# Create model with word2vec and our corpus of tweets\n",
    "model = Word2Vec(corpus, workers=4, min_count=1, window=10, sample=1e-3)\n",
    "\n",
    "# Vectorize tweets\n",
    "X = vectorize(lines, model)\n",
    "\n",
    "# Apply TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Plot TSNE\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
