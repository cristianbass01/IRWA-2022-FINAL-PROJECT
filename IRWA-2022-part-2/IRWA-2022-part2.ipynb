{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2009a40c",
   "metadata": {},
   "source": [
    "Nil Agell u172941\n",
    "\n",
    "Jordi Badia u173484"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b29d290",
   "metadata": {},
   "source": [
    "# PART 1 - Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edec1693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jordi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d1d20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68a1be",
   "metadata": {},
   "source": [
    "Abrimos el documento json y leemos linea a linea. Una vez leidas las lineas, cargamos el json y podremos comprobar que hay un total de 400 tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0020c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 4000\n"
     ]
    }
   ],
   "source": [
    "docs_path = '/Users/jordi/Documents/UNI/4rt Curs/1r Trim/IRWA/P1-IRWA/tw_hurricane_data.json'\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "\n",
    "print('Number of tweets:', len(lines))\n",
    "lines = [json.loads(l) for l in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba002b",
   "metadata": {},
   "source": [
    "Tambien abrimos el documento csv, que nos permitir√° hacer un map de los tweets id con los document ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74340a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jordi\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>1575918182698979328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_2</td>\n",
       "      <td>1575918151862304768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_3</td>\n",
       "      <td>1575918140839673873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_4</td>\n",
       "      <td>1575918135009738752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_5</td>\n",
       "      <td>1575918119251419136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc                   id\n",
       "0  doc_1  1575918182698979328\n",
       "1  doc_2  1575918151862304768\n",
       "2  doc_3  1575918140839673873\n",
       "3  doc_4  1575918135009738752\n",
       "4  doc_5  1575918119251419136"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# define the dataset location\n",
    "filename = 'tweet_document_ids_map.csv'\n",
    "sep=\"\\t\"\n",
    "# Set Pandas to show all the columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Read the data as a dataframe\n",
    "data = pd.read_csv(filename,sep,names=[\"doc\", \"id\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec934a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the tweet text removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line=  line.lower() ## Transform in lowercase\n",
    "    line=  line.split() ## Tokenize the text to get a list of terms\n",
    "    tweet_text=[]\n",
    "    for word in line:\n",
    "        #let's try to maintain the links in the correct format for the last part\n",
    "        if \"https\" not in word: #we maintain the # and @ because have relevance and we delete all the punctuation\n",
    "            word = re.sub(r'[^\\w\\s#@]','', word)\n",
    "            word = re.sub(r'_','',word)\n",
    "\n",
    "        if word:\n",
    "            tweet_text.append(word) \n",
    "            \n",
    "    line=[word for word in tweet_text if not word in stop_words]  ##eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line=[stemmer.stem(word) for word in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    ## END CODE\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab907dd5",
   "metadata": {},
   "source": [
    "We create the index tfidf (also provided with the class code ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e029d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(tweets, num_documents):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    num_documents -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)  #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df = defaultdict(int)  #document frequencies of terms in the input document\n",
    "    idf = defaultdict(float)\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tw_id = tweet['id']\n",
    "        terms = build_terms(tweet['full_text'])\n",
    "        for i in range(len(data)):\n",
    "            if data['id'][i] == tw_id:\n",
    "                doc_id = data['doc'][i]\n",
    "\n",
    "        ## ===============================================================        \n",
    "        ## create the index for the **current page** and store it in current_page_index\n",
    "        ## current_doc_index ==> { ‚Äòterm1‚Äô: [current_doc, [list of positions]], ...,‚Äòterm_n‚Äô: [current_doc, [list of positions]]}\n",
    "\n",
    "\n",
    "        ## current_page_index ==> { ‚Äòweb‚Äô: [1, [0]], ‚Äòretrieval‚Äô: [1, [1,4]], ‚Äòinformation‚Äô: [1, [2]]}\n",
    "\n",
    "        ## the term ‚Äòweb‚Äô appears in document 1 in positions 0, \n",
    "        ## the term ‚Äòretrieval‚Äô appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_doc_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms):  ## terms contains tweet text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corresponding list\n",
    "                current_doc_index[term][1].append(position) \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_doc_index[term] = [doc_id, array('I', [position])]  #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        # normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_doc_index.items():\n",
    "            # posting will contain the list of positions for current term in current document. \n",
    "            # posting ==> [current_doc, [list of positions]] \n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_doc_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1]) / norm, 4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] += 1 # increment DF for current term\n",
    "\n",
    "        #merge the current doc index with the main index\n",
    "        for term_doc, posting_doc in current_doc_index.items():\n",
    "            index[term_doc].append(posting_doc)\n",
    "\n",
    "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_documents / df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab6fe7",
   "metadata": {},
   "source": [
    "We generate the new indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea7e3c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the TD-IDF index: 277.7 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "num_documents = len(lines)\n",
    "index, tf, df, idf = create_index_tfidf(lines, num_documents)\n",
    "print(\"Total time to create the TD-IDF index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b84f71",
   "metadata": {},
   "source": [
    "We rank the documents with the provided function in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9a4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(terms, docs, index, idf, tf):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        # TODO: check how to vectorize the query\n",
    "        # query_vector[termIndex]=idf[term]  # original\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex] = query_terms_count[term] / query_norm * idf[term]\n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "    \n",
    "    doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c5034",
   "metadata": {},
   "source": [
    "We get the ranked list of docs by getting the id of the docs that contain the query term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce8877ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
    "            term_docs = [posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union term_docs\n",
    "            docs |= set(term_docs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs = rank_documents(query, docs, index, idf, tf)\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3052e0ad",
   "metadata": {},
   "source": [
    "We create an auxiliary function to list the hashtags of a tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "392c079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_hashtags(tweet):\n",
    "    hashtags = []\n",
    "    for tag in tweet[\"entities\"][\"hashtags\"]: #we loop the hashtags of the tweet that we can find in tweet[\"entities\"][\"hashtags\"]\n",
    "        hashtags.append(\"#\"+tag[\"text\"])\n",
    "    return ' '.join(hashtags).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8f633",
   "metadata": {},
   "source": [
    "We create a function to display the relevant information of the tweet ( we also added the document to see it works properly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2088a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_display(tweets, idd,data):\n",
    "    for i in range(len(data)):\n",
    "        if data['doc'][i] == idd: #we map the doc_id to the tweet_id\n",
    "            doc = data['doc'][i]\n",
    "            tw_id = data['id'][i]\n",
    "            for tweet in tweets: #we loop all tweets until we find the one with the corresponding tweet_id, then we display\n",
    "                if (tweet['id']) == tw_id:\n",
    "                    return\"Document : \"+ str(doc)+  \"|\" +\" Tweet: \" + str(tweet['full_text']) + \"|\" + \"Username: \" + str(tweet[\"user\"][\"name\"]) + \"|\" + \"Date: \"+ str(tweet[\"created_at\"]) + \"|\" + \"Hashtags: \" + list_hashtags(tweet) + \"|\" + \"Likes: \" +  str(tweet[\"favorite_count\"]) + \"|\" + \"Retweets: \"+ str(tweet[\"retweet_count\"]) + \"|\" + \"Url: \" + \"twitter.com/\"+str(tweet[\"user\"][\"id\"])+\"/status/\"+tweet['id_str']+\"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0182426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "computer\n",
      "\n",
      "======================\n",
      "Top 10 results out of 3 for the searched query:\n",
      "\n",
      "Document : doc_1798| Tweet: Stuck in WA state due to #hurricaneian - Damage assessment has commenced through the use of @CrisisTrack on my 12-yr old‚Äôs school computer. üòÇ a week trip turned into a two week trip. #paradisefloodie https://t.co/KiCwlOYIRy|Username: CFM Flood Mgr|Date: Fri Sep 30 16:45:21 +0000 2022|Hashtags: #hurricaneian #paradisefloodie|Likes: 1|Retweets: 0|Url: twitter.com/849291804390309889/status/1575889546101035008\n",
      "\n",
      "Document : doc_3331| Tweet: #JunkScience \"study\" sez \"man-caused\" #GlobalWarming added 10% rain to #HurricaneIan?\n",
      "‚Ä¢Computer simulation? Same scam that saw 2.2Œú #Covid dead in US?\n",
      "‚Ä¢Gov't lab\n",
      "‚Ä¢Not peer-reviewed\n",
      "‚Ä¢Pimped by #FakeNews @AP \n",
      "‚Ä¢You'd buy eco-dictatorship \"to save earth\"?\n",
      "https://t.co/ztid0sqLhK|Username: The Fraser Faithful|Date: Fri Sep 30 15:08:32 +0000 2022|Hashtags: #JunkScience #GlobalWarming #HurricaneIan #Covid #FakeNews|Likes: 2|Retweets: 2|Url: twitter.com/875111535517028357/status/1575865182944894978\n",
      "\n",
      "Document : doc_3900| Tweet: HURRICANE IAN RECOVERY: Christian Tech Center Ministries is pleased to announce the launch of our #HurricaneIan tech recovery program to assist local households affected by this storm. If your computer has been damaged or is no longer working as a result of Ian's impact, https://t.co/WIofQW4nzc|Username: Christian Tech Center Ministries|Date: Fri Sep 30 14:38:22 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/1333552175004475397/status/1575857591602712576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(tweet_display(lines, d_id,data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c892ef",
   "metadata": {},
   "source": [
    "# 5 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12ab8ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 10 results out of 261 for the searched query:\n",
      "\n",
      "Document : doc_1493| Tweet: It‚Äôs not the wind that‚Äôs so bad for us in the lowcountry ; like in Florida it‚Äôs the wind I worry about. Here it‚Äôs the FLOODING. The lowcountry floods during regular rain storms; hurricanes can flood out so many homes so fast here. #HurricaneIan|Username: qaatilüê∫ | #BlackRiddler üé≤|Date: Fri Sep 30 17:14:34 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/958535964/status/1575896898481053697\n",
      "\n",
      "Document : doc_2488| Tweet: If you are in a flood zone, you will be required by your mortgage company to carry a flood policy.  Here is what it covers. \n",
      "#HurricaneIan #floodinsurance https://t.co/FEiq7SI4cq|Username: Tony Tyan|Date: Fri Sep 30 15:51:01 +0000 2022|Hashtags: #HurricaneIan #floodinsurance|Likes: 0|Retweets: 0|Url: twitter.com/1551356616/status/1575875872934232064\n",
      "\n",
      "Document : doc_1862| Tweet: This is near where I live. Flood didn't affect me. Caused by flooding of #EconRiver #EconTrail #HurricaneIan #FloridaLife https://t.co/G1r0teVGAj|Username: Nydia needs coffee #FullofCoffee ‚òïüáµüá∑|Date: Fri Sep 30 16:38:49 +0000 2022|Hashtags: #EconRiver #EconTrail #HurricaneIan #FloridaLife|Likes: 7|Retweets: 2|Url: twitter.com/935229740851646464/status/1575887905234776064\n",
      "\n",
      "Document : doc_1691| Tweet: Flood Safety: If you‚Äôve been affected by flooding, please be aware of floodwater contaminants!\n",
      "\n",
      "‚ùåDo not drink floodwater \n",
      "‚ùåDo not cook, clean, or brush teeth with flood water \n",
      "‚úîÔ∏èCover open wounds \n",
      "‚úîÔ∏èLimit exposure to floodwater\n",
      "\n",
      "#HurricaneIan https://t.co/GPIF3Oj7mP|Username: Healthcare Ready|Date: Fri Sep 30 16:56:05 +0000 2022|Hashtags: #HurricaneIan|Likes: 1|Retweets: 2|Url: twitter.com/83664766/status/1575892248319119360\n",
      "\n",
      "Document : doc_3960| Tweet: Flash Flood warning for coastal Georgetown county until 1:30 pm. 2\"-4\" of rain has already fallen with an additional 2\" to 3\" possible. Flash flooding is ongoing or expected to begin shortly. Be safe and NEVER travel/drive through flooded streets. #SCwx .@WBTWNews13 #HurricaneIan https://t.co/A5CmYPg3tc|Username: James Hopkins|Date: Fri Sep 30 14:34:58 +0000 2022|Hashtags: #SCwx #HurricaneIan|Likes: 5|Retweets: 3|Url: twitter.com/19933511/status/1575856735734190081\n",
      "\n",
      "Document : doc_857| Tweet: Flooding outside Iona. #HurricaneIan https://t.co/ZIMzpJE2ge|Username: Kimberly Leonard|Date: Fri Sep 30 17:59:56 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/52282776/status/1575908317553213442\n",
      "\n",
      "Document : doc_1672| Tweet: Edgewater Florida Flooding. #HurricaneIan https://t.co/eK3lFQ0yLq|Username: tuddle|Date: Fri Sep 30 16:58:21 +0000 2022|Hashtags: #HurricaneIan|Likes: 4|Retweets: 0|Url: twitter.com/15170752/status/1575892817770999808\n",
      "\n",
      "Document : doc_3488| Tweet: Power is staring to flicker on and off and the pool is about to flood over #HurricaneIan|Username: Eva|Date: Fri Sep 30 15:01:13 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/394258538/status/1575863343080157185\n",
      "\n",
      "Document : doc_3471| Tweet: #HurricaneIan Flooded Angelo Dawkins' Home \n",
      "\n",
      "https://t.co/DFqk1X1dgZ|Username: Ringside News|Date: Fri Sep 30 15:02:11 +0000 2022|Hashtags: #HurricaneIan|Likes: 73|Retweets: 8|Url: twitter.com/1163427092/status/1575863585452048384\n",
      "\n",
      "Document : doc_3470| Tweet: #HurricaneIan Flooded Angelo Dawkins' Home \n",
      "\n",
      "https://t.co/XKve9Y21oo|Username: ‚≠êÔ∏èPWM - WWE & AEW News & Rumors‚≠êÔ∏è|Date: Fri Sep 30 15:02:11 +0000 2022|Hashtags: #HurricaneIan|Likes: 3|Retweets: 0|Url: twitter.com/200978998/status/1575863585460420608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "query = 'flood'\n",
    "ranked_docs = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(tweet_display(lines, d_id,data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3796c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 10 results out of 95 for the searched query:\n",
      "\n",
      "Document : doc_2682| Tweet: Our thoughts and prayers are with the people of Florida. Members, please continue to check your email for important updates from the VTA about emergency waivers and @GovernorVA 's state of emergency declaration as we prepare for #HurricaneIan.\n",
      "https://t.co/BXXJMxed3J|Username: Va Trucking Assn.|Date: Fri Sep 30 15:40:56 +0000 2022|Hashtags: #HurricaneIan|Likes: 1|Retweets: 1|Url: twitter.com/80934149/status/1575873336596647937\n",
      "\n",
      "Document : doc_588| Tweet: Disaster-Preparedness Training Expert and author, Haskell Moore, says, ‚Äúsave your phone for emergency communications and have two trusty flashlights minimum is my advice in any emergency,‚Äù Read more on the blog. https://t.co/Ajy4q4KZ5O\n",
      "#maglite\n",
      "#flashlite\n",
      "#hurricaneian|Username: MAGLITE|Date: Fri Sep 30 18:10:01 +0000 2022|Hashtags: #maglite #flashlite #hurricaneian|Likes: 2|Retweets: 1|Url: twitter.com/2774222486/status/1575910855274905614\n",
      "\n",
      "Document : doc_1911| Tweet: Ilham Thurston of LSC‚Äôs Disaster Team is in Raleigh responding to Hurricane Ian. The Emergency Operations Center has been activated, and Ilham will be on call to respond alongside North Carolina Emergency Management. #LSC #hurricaneian https://t.co/ps50f4y6HC|Username: Lutheran Services Carolinas|Date: Fri Sep 30 16:34:56 +0000 2022|Hashtags: #LSC #hurricaneian|Likes: 0|Retweets: 1|Url: twitter.com/1192108853632098304/status/1575886925600169984\n",
      "\n",
      "Document : doc_582| Tweet: @NWS South Carolinian's\n",
      "Follow your states Emergency Advisories\n",
      "&amp; Remember \"Turn Around Don't Drown\"\n",
      "\n",
      "State of Emergency for\n",
      "S Carolina @SCEMD\n",
      "SC Dept of Trans @SCDOTPress\n",
      "\n",
      "@NHC_Atlantic\n",
      "@fema\n",
      "@RedCross\n",
      "\n",
      "#HurricaneIan \n",
      "#hurricanequestions \n",
      "#Ian\n",
      "https://t.co/VronBdajgh|Username: JCTrevino|Date: Fri Sep 30 18:10:12 +0000 2022|Hashtags: #HurricaneIan #hurricanequestions #Ian|Likes: 1|Retweets: 0|Url: twitter.com/1288241341986754561/status/1575910899634032640\n",
      "\n",
      "Document : doc_1674| Tweet: @NWSCharlestonSC @SCEMD South Carolinian's\n",
      "Follow your states Emergency Advisories\n",
      "&amp; Remember \"Turn Around Don't Drown\"\n",
      "\n",
      "State of Emergency for\n",
      "S Carolina @SCEMD\n",
      "@NHC_Atlantic\n",
      "@fema\n",
      "@RedCross\n",
      " @horrycountypd\n",
      "\n",
      "#HurricaneIan \n",
      "#hurricanequestions \n",
      "#Ian\n",
      "https://t.co/djw9wX8chL|Username: JCTrevino|Date: Fri Sep 30 16:58:11 +0000 2022|Hashtags: #HurricaneIan #hurricanequestions #Ian|Likes: 0|Retweets: 0|Url: twitter.com/1288241341986754561/status/1575892777048494080\n",
      "\n",
      "Document : doc_143| Tweet: @EdPiotrowski South Carolinian's\n",
      "Follow your states Emergency Advisories\n",
      "&amp; Remember \"Turn Around Don't Drown\"\n",
      "\n",
      "State of Emergency for\n",
      "S Carolina @SCEMD\n",
      "SC Dept of Trans @SCDOTPress\n",
      "\n",
      "@NHC_Atlantic\n",
      "@fema\n",
      "@RedCross\n",
      "\n",
      "#HurricaneIan \n",
      "#hurricanequestions \n",
      "#Ian\n",
      "https://t.co/y3cxdlWiJp|Username: JCTrevino|Date: Fri Sep 30 18:31:31 +0000 2022|Hashtags: #HurricaneIan #hurricanequestions #Ian|Likes: 0|Retweets: 0|Url: twitter.com/1288241341986754561/status/1575916265285513216\n",
      "\n",
      "Document : doc_1882| Tweet: @NWSSPC Please tune into your local weather \n",
      "Follow your states Emergency Advisories\n",
      "\n",
      "States of Emergency for\n",
      "Georgia @GeorgiaEMAHS\n",
      "S Carolina @SCEMD\n",
      "N Carolina @NCPublicSafety\n",
      "Virginia @VDEM\n",
      "#HurricaneIan \n",
      "#hurricanequestions \n",
      "#Ian\n",
      "@NHC_Atlantic\n",
      "@fema\n",
      "@RedCross\n",
      "https://t.co/7GoiLh6BVe|Username: JCTrevino|Date: Fri Sep 30 16:37:18 +0000 2022|Hashtags: #HurricaneIan #hurricanequestions #Ian|Likes: 0|Retweets: 0|Url: twitter.com/1288241341986754561/status/1575887523074768896\n",
      "\n",
      "Document : doc_1685| Tweet: @horrycountypd @SCEMD South Carolinian's\n",
      "Follow your states Emergency Advisories\n",
      "&amp; Remember \"Turn Around Don't Drown\"\n",
      "\n",
      "State of Emergency for\n",
      "S Carolina @SCEMD\n",
      "\n",
      "@NHC_Atlantic\n",
      "@fema\n",
      "@RedCross \n",
      "@horrycountypd\n",
      "\n",
      "#HurricaneIan \n",
      "#hurricanequestions \n",
      "#Ian\n",
      "\n",
      "https://t.co/eIFnnCkkmy|Username: JCTrevino|Date: Fri Sep 30 16:56:35 +0000 2022|Hashtags: #HurricaneIan #hurricanequestions #Ian|Likes: 0|Retweets: 0|Url: twitter.com/1288241341986754561/status/1575892373321572373\n",
      "\n",
      "Document : doc_2319| Tweet: FL - REGULATORY BULLETIN - Emergency Order: #HurricaneIan #PropertyLegalNews \n",
      "https://t.co/FdymEyXkhK https://t.co/7IUu3E3nj8|Username: PLRB|Date: Fri Sep 30 16:00:43 +0000 2022|Hashtags: #HurricaneIan #PropertyLegalNews|Likes: 0|Retweets: 0|Url: twitter.com/240695548/status/1575878315675717632\n",
      "\n",
      "Document : doc_375| Tweet: @horrycountypd @SCEMD South Carolinian's\n",
      "Follow your states Emergency Advisories\n",
      "Remember \"Turn Around Don't Drown\"\n",
      "\n",
      "State of Emergency for\n",
      "S Carolina @SCEMD\n",
      "SC Dept of Trans @SCDOTPress\n",
      "\n",
      "@horrycountypd\n",
      "@NHC_Atlantic \n",
      "@fema \n",
      "@RedCross\n",
      "\n",
      "#HurricaneIan \n",
      "#hurricanequestions \n",
      "#Ian\n",
      "https://t.co/pQgfQJYgh4|Username: JCTrevino|Date: Fri Sep 30 18:20:58 +0000 2022|Hashtags: #HurricaneIan #hurricanequestions #Ian|Likes: 1|Retweets: 0|Url: twitter.com/1288241341986754561/status/1575913610983133184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "query = 'emergency'\n",
    "ranked_docs = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(tweet_display(lines, d_id,data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9947c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 10 results out of 631 for the searched query:\n",
      "\n",
      "Document : doc_3218| Tweet: If the death doll isn't more than hurricane katrina, its pretty damn close. #HurricaneIan was absolutely worse than hurricane charley for Florida..and was THE STRONGEST hurricane in recorded history for our florida peninsula.|Username: Emily Bailee|Date: Fri Sep 30 15:14:25 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/45058299/status/1575866662070484994\n",
      "\n",
      "Document : doc_640| Tweet: Hurricane Ian before and after #HurricaneIan https://t.co/XZstkI2pN2|Username: kaden fields ‚ú™|Date: Fri Sep 30 18:07:50 +0000 2022|Hashtags: #HurricaneIan|Likes: 2|Retweets: 0|Url: twitter.com/1329640781649416197/status/1575910304159977472\n",
      "\n",
      "Document : doc_3845| Tweet: So please y'all, hold off on the severe hurricane jokes because this hurricane isn't a joke right now. And it never will be. #hurricaneian|Username: UNDERCUT SEONGHWA ENTHUSIAST ‚ÄºÔ∏èü§©|Date: Fri Sep 30 14:41:02 +0000 2022|Hashtags: #hurricaneian|Likes: 0|Retweets: 0|Url: twitter.com/1371237552737419265/status/1575858260837797888\n",
      "\n",
      "Document : doc_3336| Tweet: Although the Hurricane has finally passed, my thoughts and prayers go out to everyone trying to rebuild and recover from Hurricane Ian. \n",
      "\n",
      "#HurricaneIan üå™|Username: Tuck Beck|Date: Fri Sep 30 15:07:56 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 1|Url: twitter.com/278727661/status/1575865031207825408\n",
      "\n",
      "Document : doc_2537| Tweet: PRAY üôè FOR ALL WHO SUFFERED FROM THE HURRICANE #HurricaneIan|Username: Gamingforlife|Date: Fri Sep 30 15:48:48 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/1565775847960739841/status/1575875317918748679\n",
      "\n",
      "Document : doc_2140| Tweet: #hurricaneian\n",
      "The perpetual hurricane Ian|Username: üí•PermanentWave|Date: Fri Sep 30 16:14:34 +0000 2022|Hashtags: #hurricaneian|Likes: 0|Retweets: 0|Url: twitter.com/75442215/status/1575881799267295234\n",
      "\n",
      "Document : doc_1217| Tweet: Hurricane Ian on tourüò≠\n",
      "#HurricaneIan|Username: New York City|Date: Fri Sep 30 17:37:34 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/1068652828661559297/status/1575902689040666626\n",
      "\n",
      "Document : doc_495| Tweet: Hurricane Ian be like \n",
      "\n",
      "‚ÄúYou all haven‚Äôt had any hurricanes around here lately so I‚Äôm just gonna take my time on the East Coast.‚Äù\n",
      "\n",
      "#HurricaneIan #Ian #Hurricane|Username: Correcting your üí©|Date: Fri Sep 30 18:14:39 +0000 2022|Hashtags: #HurricaneIan #Ian #Hurricane|Likes: 0|Retweets: 0|Url: twitter.com/1565783598543806472/status/1575912020368449536\n",
      "\n",
      "Document : doc_1428| Tweet: BTW Florida's combative, anti-education/women/liberty gov DiSantis voted against hurricane relief money after Hurricane Sandy. think on that a moment. \n",
      "#HurricaneIan #Fla|Username: Butter Emailsüåªüåª|Date: Fri Sep 30 17:21:56 +0000 2022|Hashtags: #HurricaneIan #Fla|Likes: 0|Retweets: 0|Url: twitter.com/17578329/status/1575898753826856960\n",
      "\n",
      "Document : doc_746| Tweet: Yesterday, Hurricane Ian made landfall near Cayo Costa, Fla., as a Category 4 hurricane, bringing record wind and storm surge to a four-county area in Southwest Florida. Read the full update on Hurricane Ian from our MWC Florida team here:\n",
      "\n",
      "#hurricaneian #florida|Username: McGuireWoods Consulting|Date: Fri Sep 30 18:03:17 +0000 2022|Hashtags: #hurricaneian #florida|Likes: 0|Retweets: 0|Url: twitter.com/488644188/status/1575909158746038277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "query = 'hurricane'\n",
    "ranked_docs = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(tweet_display(lines, d_id,data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b238fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 10 results out of 547 for the searched query:\n",
      "\n",
      "Document : doc_2072| Tweet: Donate to the Florida Disaster Fund - Volunteer Florida to help Floridians recover from #hurricaneian  https://t.co/Ud4BNM81EJ|Username: üá∫üá∏ Amy‚ò¢Ô∏èREDACTED‚ò¢Ô∏è MAGArino ü§åüèªüá∫üá∏|Date: Fri Sep 30 16:19:57 +0000 2022|Hashtags: #hurricaneian|Likes: 1|Retweets: 0|Url: twitter.com/2398138445/status/1575883154153414656\n",
      "\n",
      "Document : doc_1700| Tweet: The current death toll in Florida sits at 21. About 10,000 people in southwest Florida are unaccounted for. üíî #hurricaneian|Username: School‚Äôs Out|Date: Fri Sep 30 16:55:15 +0000 2022|Hashtags: #hurricaneian|Likes: 0|Retweets: 0|Url: twitter.com/1420132505546465280/status/1575892040541298690\n",
      "\n",
      "Document : doc_1701| Tweet: Update: #hurricaneian has passed Florida and is now headed to South Carolina.\n",
      "\n",
      "1.8 million remain without power in Florida.|Username: TheLatestBlock.com|Date: Fri Sep 30 16:55:12 +0000 2022|Hashtags: #hurricaneian|Likes: 0|Retweets: 0|Url: twitter.com/1375638432341446656/status/1575892025312002048\n",
      "\n",
      "Document : doc_3964| Tweet: Duke Energy donates $100,000 to the Florida Disaster Fund, managed by Volunteer Florida Foundation, to assist communities affected by #HurricaneIan. To contribute to the Florida Disaster Fund, visit https://t.co/zx6l40CRQv or text DISASTER to 20222. Info: https://t.co/oAmCDqYkVQ https://t.co/UVsnxWHWGu|Username: Duke Energy|Date: Fri Sep 30 14:34:42 +0000 2022|Hashtags: #HurricaneIan|Likes: 13|Retweets: 11|Url: twitter.com/81192849/status/1575856667304071170\n",
      "\n",
      "Document : doc_563| Tweet: Florida boss called Hurricane Ian a \"nothing burger\" ‚Äî urged staff to keep working: reports .. https://t.co/FOw04OW275::: Clearwater Florida #HurricaneIan https://t.co/zDbw2By1fk|Username: WTB Research|Date: Fri Sep 30 18:11:08 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 1|Url: twitter.com/991187411630780416/status/1575911137937326081\n",
      "\n",
      "Document : doc_2506| Tweet: @MrMatthewTodd After, Florida is totally under water.\n",
      "Then, it will hit them. ü§î\n",
      "Climate change will take more and more costal lands.\n",
      "Florida is on it's way OUT. üåßüåÄ\n",
      "#HurricaneIan \n",
      "#GlobalWarming \n",
      "#ClimateChange \n",
      "#Florida \n",
      "#SouthCarolina|Username: Jennifer Coleman|Date: Fri Sep 30 15:50:08 +0000 2022|Hashtags: #HurricaneIan #GlobalWarming #ClimateChange #Florida #SouthCarolina|Likes: 0|Retweets: 0|Url: twitter.com/2218000892/status/1575875654108811267\n",
      "\n",
      "Document : doc_2261| Tweet: @GovRonDeSantis said thousands of Florida residents will need help rebuilding after #hurricaneian and urged those that could to contribute to the Florida Disaster Fund ( https://t.co/th8d863roq ) https://t.co/yWKGgJ7hMm|Username: Chip Barnett|Date: Fri Sep 30 16:04:32 +0000 2022|Hashtags: #hurricaneian|Likes: 0|Retweets: 0|Url: twitter.com/159096380/status/1575879277262032896\n",
      "\n",
      "Document : doc_3114| Tweet: Hey Florida..  lookie here..  #HurricaneIan https://t.co/JHJoHz5pd0|Username: free2bhuman|Date: Fri Sep 30 15:19:53 +0000 2022|Hashtags: #HurricaneIan|Likes: 1|Retweets: 0|Url: twitter.com/384077570/status/1575868040931323904\n",
      "\n",
      "Document : doc_2905| Tweet: Hurricane Ian heads towards South Carolina after striking Florida.\n",
      "The death toll in Florida has risen to 21. We are praying for all those affected. Our hearts are with you. #HurricaneIan https://t.co/RqvvtW7Mt5|Username: lyfmail.com|Date: Fri Sep 30 15:29:30 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/1543603637817331713/status/1575870458964344833\n",
      "\n",
      "Document : doc_2899| Tweet: They are rebuilding Florida. \n",
      "Why? \n",
      "#ClimateEmergency #HurricaneIan https://t.co/XNaKn7m7lg|Username: üò∑Miriam O'Callaghan Has a Deadline|Date: Fri Sep 30 15:29:48 +0000 2022|Hashtags: #ClimateEmergency #HurricaneIan|Likes: 4|Retweets: 3|Url: twitter.com/1031872658575642624/status/1575870535653011456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "query = 'florida'\n",
    "ranked_docs = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(tweet_display(lines, d_id,data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00805ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Top 10 results out of 238 for the searched query:\n",
      "\n",
      "Document : doc_399| Tweet: #HurricaneIan has made landfall!|Username: WeatherMAX National|Date: Fri Sep 30 18:19:30 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/1425534305619546118/status/1575913241116803072\n",
      "\n",
      "Document : doc_100| Tweet: Final #Ian and Charley comparison @HeatherZWeather, I promise! üòÇ\n",
      "\n",
      "After nearly identical landfall points in FL, #HurricaneIan's second landfall üåÄ in Georgetown, SC is only 24 miles from Charley's landfall in SC.\n",
      "\n",
      "@weatherchannel https://t.co/PvVOmy5Xhu|Username: Jason Disharoon|Date: Fri Sep 30 18:33:40 +0000 2022|Hashtags: #Ian #HurricaneIan|Likes: 5|Retweets: 1|Url: twitter.com/37661008/status/1575916807927795712\n",
      "\n",
      "Document : doc_245| Tweet: 4th Landfall !!! #HurricaneIan https://t.co/RjDkz7HbEb|Username: Najam ŸÜÿ¨ŸÖ|Date: Fri Sep 30 18:26:30 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/712763496/status/1575915004590977024\n",
      "\n",
      "Document : doc_12| Tweet: #HurricaneIan officially made landfall at 2:15 pm near Georgetown, SC.\n",
      "\n",
      "Ian made landfall as a CAT 1 with sustained winds of 85 mph.\n",
      "\n",
      "This is Ian's 3rd landfall.\n",
      "\n",
      "@SpecNews1RDU \n",
      "\n",
      "@SpecNews1ILM \n",
      "\n",
      "@SpecNews1Triad \n",
      "\n",
      "@SpecNews1CLT \n",
      "\n",
      "@SpecNews1MTN https://t.co/BvcQeOLkBG|Username: Meteorologist Vernon Turner|Date: Fri Sep 30 18:38:34 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/3232685437/status/1575918038196666392\n",
      "\n",
      "Document : doc_314| Tweet: THE 3rd and Final Landfall of #HurricaneIan just occurred S of Myrtle Beach. The storm actually intensified as pressure dropped before landfall at 977mb. #scwx https://t.co/As1LR1r2vq|Username: I65Wx (BWG/SoKy)|Date: Fri Sep 30 18:23:34 +0000 2022|Hashtags: #HurricaneIan #scwx|Likes: 2|Retweets: 0|Url: twitter.com/1371119451454763010/status/1575914266473140225\n",
      "\n",
      "Document : doc_386| Tweet: JUST IN: #HurricaneIan makes landfall near Georgetown.|Username: Kristina Lobo|Date: Fri Sep 30 18:20:07 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 0|Url: twitter.com/1260754170/status/1575913396373012492\n",
      "\n",
      "Document : doc_3227| Tweet: #HurricaneIan is starting to make landfall here in #NC.|Username: OnTheMoon, AntiRugs.Eth | GetRichOrRuggedTryin.Eth|Date: Fri Sep 30 15:13:53 +0000 2022|Hashtags: #HurricaneIan #NC|Likes: 2|Retweets: 0|Url: twitter.com/1395187526537826310/status/1575866530335629312\n",
      "\n",
      "Document : doc_1163| Tweet: Ian is making landfall again. #HurricaneIan https://t.co/NCxy8gqWLD|Username: Twisted Skies|Date: Fri Sep 30 17:40:35 +0000 2022|Hashtags: #HurricaneIan|Likes: 2|Retweets: 1|Url: twitter.com/2856918817/status/1575903449405067264\n",
      "\n",
      "Document : doc_410| Tweet: #HurricaneIan just made landfall near Georgetown, SC|Username: Jennifer Van Laar|Date: Fri Sep 30 18:18:44 +0000 2022|Hashtags: #HurricaneIan|Likes: 3|Retweets: 0|Url: twitter.com/775137023979958272/status/1575913047171813376\n",
      "\n",
      "Document : doc_3971| Tweet: #HurricaneIan as seen by Sentinel 3 just before landfall. https://t.co/b52i3g5blu|Username: Remy Mermelstein | WeatherInTheHud|Date: Fri Sep 30 14:34:19 +0000 2022|Hashtags: #HurricaneIan|Likes: 0|Retweets: 1|Url: twitter.com/3438631077/status/1575856571783184385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "query = 'landfall'\n",
    "ranked_docs = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(tweet_display(lines, d_id,data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73623acd",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85837622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>doc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.637926</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.824241</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.358856</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.096755</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.268338</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  doc_id  predicted_relevance  doc_score\n",
       "0         0       0            -0.637926        2.0\n",
       "1         0       1            -0.824241        1.0\n",
       "2         0       2            -1.358856        3.0\n",
       "3         0       3            -0.096755        1.0\n",
       "4         0       4            -1.268338        0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = pd.read_csv(\"test_predictions.csv\")\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "decb5906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>doc_score</th>\n",
       "      <th>is_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.637926</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.824241</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.358856</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.096755</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.268338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  doc_id  predicted_relevance  doc_score  is_relevant\n",
       "0         0       0            -0.637926        2.0            1\n",
       "1         0       1            -0.824241        1.0            0\n",
       "2         0       2            -1.358856        3.0            1\n",
       "3         0       3            -0.096755        1.0            0\n",
       "4         0       4            -1.268338        0.0            0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[\"is_relevant\"] = search_results[\"doc_score\"].apply(lambda y: 1 if y>=2 else 0)\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c469a",
   "metadata": {},
   "source": [
    "Evaluamos mediante los algoritmos de evaluaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6c858de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(doc_score, y_score, k=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    precision @k : float\n",
    "\n",
    "    \"\"\"\n",
    "    order = np.argsort(doc_score)[::-1]\n",
    "    doc_score = np.take(doc_score, order[:k]) #hay que ordenar los predicted igual que los del ground truth. Align binary relevance to the \n",
    "    relevant = sum(doc_score==1) #get number of relevant docs\n",
    "    return float(relevant)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f02ef80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Precision@5: 1.0\n",
      "\n",
      "\n",
      "Check on the dataset sorted by score:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>predicted_relevance</th>\n",
       "      <th>doc_score</th>\n",
       "      <th>is_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>1.705258</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>1.116369</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>1.096797</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1.084367</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>1.082985</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     query_id  doc_id  predicted_relevance  doc_score  is_relevant\n",
       "88          0      88             1.705258        2.0            1\n",
       "114         0     114             1.116369        2.0            1\n",
       "63          0      63             1.096797        1.0            0\n",
       "34          0      34             1.084367        1.0            0\n",
       "86          0      86             1.082985        3.0            1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for query 0\n",
    "\n",
    "current_query = 0\n",
    "current_query_res = search_results[search_results[\"query_id\"] == current_query]\n",
    "\n",
    "k = 5\n",
    "print(\"==> Precision@{}: {}\\n\".format(k, precision_at_k(current_query_res[\"is_relevant\"], current_query_res[\"predicted_relevance\"], k)))\n",
    "print(\"\\nCheck on the dataset sorted by score:\\n\")\n",
    "\n",
    "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0bfe3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_precision_at_k(doc_score, y_score, k=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    average precision @k : float\n",
    "    \"\"\"\n",
    "    gtp = np.sum(doc_score==1) \n",
    "    order = y_score.argsort()[::-1]\n",
    "    doc_score = np.take(doc_score, order[:k])\n",
    "    ## if all documents are not relevant\n",
    "    if gtp == 0:\n",
    "        return 0\n",
    "    n_relevant_at_i = 0\n",
    "    prec_at_i = 0\n",
    "    for i in range(len(doc_score)):\n",
    "        if doc_score[i] == 1 :\n",
    "            n_relevant_at_i += 1\n",
    "            prec_at_i +=  precision_at_k(doc_score, y_score, i+1)\n",
    "    return prec_at_i/gtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb626341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7539696295259657"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_precision_at_k(np.array(current_query_res[\"is_relevant\"]), np.array(current_query_res[\"predicted_relevance\"]), 150)\n",
    "#avg_precision_at_k(np.array([1,0,0,1,1,0]), np.array([0.9,0.8,0.7,0.6,0.5, 0.4]),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10a4eb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5021658287937125"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check with 'average_precision_score' of 'sklearn' library\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "k = 150\n",
    "temp = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
    "average_precision_score(np.array(temp[\"is_relevant\"]), np.array(temp[\"predicted_relevance\"][:k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6ba440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(search_res, k=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_res: search results dataset containing:\n",
    "        query_id: query id.\n",
    "        doc_id: document id.\n",
    "        predicted_relevance: relevance predicted through LightGBM.\n",
    "        doc_score: actual score of the document for the query (ground truth).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean average precision @ k : float\n",
    "    \"\"\"\n",
    "    avp = []\n",
    "    for q in search_res[\"query_id\"].unique():  # loop over all query id\n",
    "        curr_data =  search_res[search_res.query_id==q]# select data for current query\n",
    "        avp.append(avg_precision_at_k(np.array(curr_data[\"is_relevant\"]), \n",
    "                                      np.array(curr_data[\"predicted_relevance\"]),k ))  #append average precision for current query\n",
    "    return np.sum(avp)/len(avp), avp  # return mean average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec38448e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20241416587577077"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_k, avp = map_at_k(search_results, 10)\n",
    "map_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07aa7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_at_k(doc_score, y_score, k=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Reciprocal Rank for qurrent query\n",
    "    \"\"\"\n",
    "\n",
    "    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order. As before\n",
    "    doc_score = np.take(doc_score, order[\n",
    "                             :k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k. As before\n",
    "    if np.sum(doc_score) == 0:  # if there are not relevant doument return 0\n",
    "        return 0\n",
    "    return 1 / (np.argmax(doc_score == 1) + 1)  # hint: to get the position of the first relevant document use \"np.argmax\" (+1 because the idex starts from 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7238351e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr_at_k(np.array(current_query_res[\"is_relevant\"]), np.array(current_query_res[\"predicted_relevance\"]), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d750ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(doc_score, y_score, k=10): #doc_scire are the labels (ground truth) and y_score are the system scores\n",
    "    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order.\n",
    "    doc_score = np.take(doc_score, order[:k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    gain = 2 ** doc_score - 1  # First we calculate the upper part of the formula which is the CG (use formula 7 above) (notice it is based on the ground truth relevance)\n",
    "    discounts = np.log2(np.arange(len(doc_score)) + 2)  # Compute denominator (np.arrange creates a list of numbers betweeen 0 and len(doc_score)-1), then the + 2 addresses the fact that the numbers start from 0\n",
    "    return np.sum(gain / discounts)  #return dcg@k\n",
    "\n",
    "\n",
    "def ndcg_at_k(doc_score, y_score, k=10):\n",
    "    dcg_max = dcg_at_k(doc_score, doc_score, k) #ideal dcg\n",
    "    #print(dcg_max)\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return np.round(dcg_at_k(doc_score, y_score, k) / dcg_max, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f730da9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@10 for query with query_id=0: 0.4392\n"
     ]
    }
   ],
   "source": [
    "query_id = 0\n",
    "k = 10\n",
    "labels = np.array(search_results[search_results['query_id'] == query_id][\"doc_score\"])\n",
    "scores = np.array(search_results[search_results['query_id'] == query_id][\"predicted_relevance\"])\n",
    "ndcg_k = np.round(ndcg_at_k(labels, scores, k), 4)\n",
    "print(\"ndcg@{} for query with query_id={}: {}\".format(k, query_id, ndcg_k))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3398195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> For Query 0 Precision@10: 1.0\n",
      "Average Precission@10 for query with q_id=0: 0.7539696295259657\n",
      "ndcg@10 for query with q_id=0: 0.4392\n",
      "\n",
      "==> For Query 1 Precision@10: 1.0\n",
      "Average Precission@10 for query with q_id=1: 0.6975119074743135\n",
      "ndcg@10 for query with q_id=1: 0.4392\n",
      "\n",
      "==> For Query 2 Precision@10: 1.0\n",
      "Average Precission@10 for query with q_id=2: 0.6551696598347273\n",
      "ndcg@10 for query with q_id=2: 0.4392\n",
      "\n",
      "==> For Query 3 Precision@10: 0.6\n",
      "Average Precission@10 for query with q_id=3: 0.6581143951833607\n",
      "ndcg@10 for query with q_id=3: 0.4392\n",
      "\n",
      "==> For Query 4 Precision@10: 1.0\n",
      "Average Precission@10 for query with q_id=4: 0.7473673937483808\n",
      "ndcg@10 for query with q_id=4: 0.4392\n",
      "\n",
      "MAP@10: 0.20241416587577077\n",
      "MRR@10: 1.0\n"
     ]
    }
   ],
   "source": [
    "#All evaluations techniques for all queries\n",
    "k = 10\n",
    "for i in range(5):\n",
    "    current_query = i\n",
    "    current_query_res = search_results[search_results[\"query_id\"] == current_query] \n",
    "    print(\"\\n==> For Query {} Precision@{}: {}\".format(current_query, k,\n",
    "                                precision_at_k(np.array(current_query_res[\"is_relevant\"]), np.array(current_query_res[\"predicted_relevance\"]), k)))\n",
    "\n",
    "    print(\"Average Precission@{} for query with q_id={}: {}\".format(k,current_query,avg_precision_at_k(np.array(current_query_res[\"is_relevant\"]), np.array(current_query_res[\"predicted_relevance\"]), 150)))\n",
    "\n",
    "    \n",
    "\n",
    "    labels = np.array(search_results[search_results['query_id'] == query_id][\"doc_score\"])\n",
    "    scores = np.array(search_results[search_results['query_id'] == query_id][\"predicted_relevance\"])\n",
    "    ndcg_k = np.round(ndcg_at_k(labels, scores, k),4)\n",
    "    print(\"ndcg@{} for query with q_id={}: {}\".format(k,current_query,ndcg_k))\n",
    "\n",
    "map_k, avp = map_at_k(search_results, k)\n",
    "print(\"\\nMAP@{}: {}\".format(k,map_k))\n",
    "\n",
    "print(\"MRR@{}: {}\".format(k,rr_at_k(np.array(current_query_res[\"is_relevant\"]), np.array(current_query_res[\"predicted_relevance\"]), k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "459cfc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jordi\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:800: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jordi\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:810: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "clean_tweets = []\n",
    "for tweet in lines: \n",
    "        #get the terms cleaned \n",
    "        clean_tweets.append(build_terms(tweet['full_text']))\n",
    "\n",
    "model = Word2Vec(clean_tweets, workers=4, min_count=50, window=10, sample=1e-3)\n",
    "\n",
    "#print (model.wv.most_similar('memory'))\n",
    "\n",
    "X = model.wv[model.wv.key_to_index]\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad6b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
